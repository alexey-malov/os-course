# Вопросы к экзамену

- [Вопросы к экзамену](#вопросы-к-экзамену)
  - [Общие сведения об ОС](#общие-сведения-об-ос)
  - [Процессы/переключение контекста и регистры, режимы процессора](#процессыпереключение-контекста-и-регистры-режимы-процессора)
  - [Память, виртуальная память и MMU, шины](#память-виртуальная-память-и-mmu-шины)
  - [Базовые абстракции ОС: процессы, адресные пространства, файлы](#базовые-абстракции-ос-процессы-адресные-пространства-файлы)
  - [Пользователи, UID/GID и модель прав](#пользователи-uidgid-и-модель-прав)
  - [Адресное пространство и аппаратная защита](#адресное-пространство-и-аппаратная-защита)
  - [Файловые системы: пути, каталоги, операции](#файловые-системы-пути-каталоги-операции)
  - [Базовые понятия: процесс, программа, абстракция ОС, параллелизм](#базовые-понятия-процесс-программа-абстракция-ос-параллелизм)
  - [Создание процессов: UNIX fork/exec и Windows CreateProcess](#создание-процессов-unix-forkexec-и-windows-createprocess)
  - [Идентификаторы и дескрипторы, управление процессами](#идентификаторы-и-дескрипторы-управление-процессами)
  - [Классическая модель потока](#классическая-модель-потока)
  - [Параллелизм и планирование, ресурсы потока](#параллелизм-и-планирование-ресурсы-потока)
  - [Жизненный цикл потоков и базовые примитивы](#жизненный-цикл-потоков-и-базовые-примитивы)
  - [Базовые понятия IPC и синхронизации](#базовые-понятия-ipc-и-синхронизации)
  - [Классические алгоритмы синхронизации](#классические-алгоритмы-синхронизации)
  - [Спинлоки](#спинлоки)
  - [Спинлоки в ядре: IRQ и вытеснение](#спинлоки-в-ядре-irq-и-вытеснение)
  - [Мьютексы: идея и реализация](#мьютексы-идея-и-реализация)
  - [Futex и condition variables](#futex-и-condition-variables)
  - [std::mutex и разновидности мьютексов](#stdmutex-и-разновидности-мьютексов)
  - [FairRWLock и мониторы](#fairrwlock-и-мониторы)
  - [Барьеры синхронизации (phase synchronization), ошибки работы с многопоточностью](#барьеры-синхронизации-phase-synchronization-ошибки-работы-с-многопоточностью)
  - [Планирование процессов, контекст планирования](#планирование-процессов-контекст-планирования)
  - [Виды процессов и их планирование](#виды-процессов-и-их-планирование)
  - [Категории систем и цели планирования](#категории-систем-и-цели-планирования)
  - [Интерактивные системы: Round Robin и приоритеты](#интерактивные-системы-round-robin-и-приоритеты)
  - [Планирование в системах реального времени](#планирование-в-системах-реального-времени)
  - [Политика против механизма, user-level потоки](#политика-против-механизма-user-level-потоки)
  - [Память как ресурс](#память-как-ресурс)
  - [Память в ранних системах](#память-в-ранних-системах)
  - [Адресные пространства](#адресные-пространства)
  - [Защита и релокация, swapping](#защита-и-релокация-swapping)
  - [Управление памятью с помощью битовых карт](#управление-памятью-с-помощью-битовых-карт)
  - [Управление памятью на основе списков сегментов](#управление-памятью-на-основе-списков-сегментов)
  - [Алгоритмы выбора «дыры» (First/Next/Best/Worst Fit)](#алгоритмы-выбора-дыры-firstnextbestworst-fit)
  - [Виртуальная память](#виртуальная-память)
  - [Устройство вирутальной памяти](#устройство-вирутальной-памяти)
  - [Переход от физических адресов к виртуальным](#переход-от-физических-адресов-к-виртуальным)
  - [Что происходит аппаратно при включении paging: pipeline flush и очистка TLB](#что-происходит-аппаратно-при-включении-paging-pipeline-flush-и-очистка-tlb)
  - [Виртуальная память в многоядерных системах](#виртуальная-память-в-многоядерных-системах)
  - [Переключение адресных пространств, кеши и безопасность](#переключение-адресных-пространств-кеши-и-безопасность)
  - [TLB и переключение контекста: flush, ASID/PCID, многоуровневые TLB](#tlb-и-переключение-контекста-flush-asidpcid-многоуровневые-tlb)
  - [Инвертированные таблицы страниц (IPT) и Radix Page Tables: эволюция масштабируемости](#инвертированные-таблицы-страниц-ipt-и-radix-page-tables-эволюция-масштабируемости)
  - [Основы замещения страниц и цели политики](#основы-замещения-страниц-и-цели-политики)
  - [Алгоритмы Optimal Page Replacement и NRU](#алгоритмы-optimal-page-replacement-и-nru)
  - [Алгоритмы FIFO и Second Chance](#алгоритмы-fifo-и-second-chance)
  - [Алгоритмы Clock и Enhanced Clock](#алгоритмы-clock-и-enhanced-clock)
  - [Алгоритмы LRU и NFU](#алгоритмы-lru-и-nfu)
  - [Алгоритмы Aging и Working Set](#алгоритмы-aging-и-working-set)
  - [Алгоритмы WSClock и гибридные алгоритмы](#алгоритмы-wsclock-и-гибридные-алгоритмы)
  - [Управление памятью на практике: базовые понятия и метрики, локальная и глобальная политика распределения страниц](#управление-памятью-на-практике-базовые-понятия-и-метрики-локальная-и-глобальная-политика-распределения-страниц)
  - [Проблема распределения страниц и управление частотой ошибок (PFF), Thrashing](#проблема-распределения-страниц-и-управление-частотой-ошибок-pff-thrashing)
  - [Реакции ОС на нехватку памяти](#реакции-ос-на-нехватку-памяти)
  - [Очистка страниц, выбор размера страницы](#очистка-страниц-выбор-размера-страницы)
  - [Раздельные адресные пространства инструкций и данных (I-space / D-space)](#раздельные-адресные-пространства-инструкций-и-данных-i-space--d-space)
  - [Разделяемые библиотеки и Memory-mapped files](#разделяемые-библиотеки-и-memory-mapped-files)
  - [Практическая реализация виртуальной памяти](#практическая-реализация-виртуальной-памяти)
  - [Виртуальная память, I/O, DMA](#виртуальная-память-io-dma)
  - [Защита страниц во время I/O, особенности работы DMA](#защита-страниц-во-время-io-особенности-работы-dma)
  - [Backing Store: swap как основа виртуальной памяти](#backing-store-swap-как-основа-виртуальной-памяти)
  - [Сегментация памяти](#сегментация-памяти)
  - [Сегментация + пейджинг](#сегментация--пейджинг)
  - [Зачем нужны файловые системы. Накопители и блочный интерфейс](#зачем-нужны-файловые-системы-накопители-и-блочный-интерфейс)
  - [Абстракция файла, имена файлов](#абстракция-файла-имена-файлов)
  - [Расширения и типы файлов, структура файлов](#расширения-и-типы-файлов-структура-файлов)
  - [Доступ к содержимому файла. Атрибуты файла](#доступ-к-содержимому-файла-атрибуты-файла)
  - [Операции над файлами. Каталоги](#операции-над-файлами-каталоги)
  - [Пути к файлам. Операции над каталогами.](#пути-к-файлам-операции-над-каталогами)
  - [Файловые ссылки](#файловые-ссылки)
  - [Внутреннее устройство файловой системы на диске. Загрузка ОС](#внутреннее-устройство-файловой-системы-на-диске-загрузка-ос)
  - [Размещение файлов. Реализация каталогов](#размещение-файлов-реализация-каталогов)
  - [Log-Structured File System](#log-structured-file-system)
  - [Переполнение лога и производительность LFS](#переполнение-лога-и-производительность-lfs)
  - [Журналирование](#журналирование)
  - [Журналирование. Детали.](#журналирование-детали)
  - [Flash/SSD. Часть 1](#flashssd-часть-1)
  - [Flash/SSD. Часть 2](#flashssd-часть-2)
  - [Virtual File System](#virtual-file-system)
  - [Windows vs UNIX/Linux](#windows-vs-unixlinux)
  - [FUSE: файловая система в userspace](#fuse-файловая-система-в-userspace)
  - [Управление файловыми системами: размер блока ⭐](#управление-файловыми-системами-размер-блока-)
  - [Управление свободным местом. Дисковые квоты](#управление-свободным-местом-дисковые-квоты)
  - [Резервное копирование - 1](#резервное-копирование---1)
  - [Резервное копирование - 2](#резервное-копирование---2)
  - [Производительность ФС](#производительность-фс)
  - [Дефрагментация, удаление файлов и шифрование](#дефрагментация-удаление-файлов-и-шифрование)
  - [Сборка приложения: компиляция, ассемблирование, линковка](#сборка-приложения-компиляция-ассемблирование-линковка)
  - [Динамические библиотеки, их плюсы и минусы.](#динамические-библиотеки-их-плюсы-и-минусы)
  - [Динамическая компоновка. Адресное пространство процесса](#динамическая-компоновка-адресное-пространство-процесса)
  - [Релокация. PIC. Безопасность](#релокация-pic-безопасность)
  - [Загрузка библиотек. Декорирование имён. ABI совместимость](#загрузка-библиотек-декорирование-имён-abi-совместимость)
  - [Проблемы с DLL. Инструменты анализа](#проблемы-с-dll-инструменты-анализа)

## Общие сведения об ОС

- Дайте определение операционной системы и объясните, какие задачи она решает в современном компьютере. ⭐
- Почему прикладные программы обычно не работают напрямую с «железом»? Какие проблемы возникли бы без ОС? ⭐⭐
- Объясните, как ОС управляет ресурсами при многопользовательской работе. Какие типы конфликтов она предотвращает? ⭐⭐
- Что такое мультиплексирование во времени и в пространстве? Приведите по 2 примера каждого. ⭐⭐
- В каких типах устройств ОС может быть не нужна? Приведите примеры и объясните почему. ⭐⭐
- Чем RTOS отличается от «полноценной» ОС общего назначения? В каких областях RTOS критична? ⭐⭐
- Опишите базовую архитектуру ПК (CPU–память–I/O) и роль системной шины в этой модели. ⭐
- Что означает, что ОС «создаёт абстракции»? Приведите пример перехода от «блоков диска» к «файлам». ⭐⭐
- Что такое драйвер устройства и почему драйверы часто выполняются в режиме ядра? ⭐⭐
- Назовите и сравните три способа установки драйверов (пересборка ядра, загрузка при старте, hotplug). ⭐⭐
- Что такое прерывание (interrupt) и зачем оно нужно при вводе-выводе? Опишите путь от устройства до обработчика. ⭐⭐⭐
- Сравните методы I/O: busy waiting, interrupts, DMA — плюсы/минусы и типичные сценарии использования. ⭐⭐⭐
- Опишите процесс загрузки компьютера: что делает BIOS/UEFI, как выбирается загрузчик и что происходит при запуске ядра ОС. ⭐⭐⭐
- Сравните BIOS и UEFI (MBR vs GPT/ESP, ограничения, возможности). Почему UEFI считают «маленькой ОС»? ⭐⭐⭐

## Процессы/переключение контекста и регистры, режимы процессора

- Что такое context switch и почему при нём важно сохранять/восстанавливать состояние процессора? ⭐⭐
- Какие регистры процессора важны для ОС (PC, SP, PSW) и как ОС использует/учитывает их? ⭐⭐
- Чем архитектура процессора отличается от микроархитектуры? Почему ОС в основном «видит» архитектуру? ⭐⭐
- Объясните, что такое конвейер (pipeline) и как он повышает производительность. Какие сложности он создаёт? ⭐⭐⭐
- Что такое суперскалярный процессор и «внеочередное выполнение»? Почему это может быть важно для ОС? ⭐⭐⭐
- Сравните kernel mode и user mode: какие инструкции/возможности доступны в каждом режиме и почему? ⭐⭐
- Что такое системный вызов (syscall) и чем он отличается от обычного вызова функции? Опишите общий механизм «trap в ядро». ⭐⭐⭐
- Какие бывают аппаратные traps (кроме syscall)? Как ОС может реагировать на исключительные ситуации? ⭐⭐

## Память, виртуальная память и MMU, шины

- Как ОС управляет памятью при одновременной работе нескольких программ? Какие цели преследуются (справедливость/защита/безопасность)? ⭐⭐
- Что такое виртуальная память и зачем она нужна? Опишите идею «RAM как кэш для диска/SSD». ⭐⭐
- Какова роль MMU? Что означает «преобразование виртуальных адресов в физические»? ⭐⭐⭐
- Что такое кэш CPU (L1/L2/L3)? Объясните термины cache hit и cache miss и влияние на производительность. ⭐⭐
- Приведите примеры кэширования в ОС (не в железе) и объясните, почему это ускоряет систему. ⭐⭐
- Сравните HDD и SSD: как устроены, почему HDD медленнее при случайном доступе, и что усложняет запись в SSD. ⭐⭐
- Что такое «шина» в архитектуре компьютера? Почему современные системы используют несколько шин вместо одной? ⭐⭐
- Чем PCIe принципиально отличается от старых параллельных общих шин (PCI/ISA)? Как масштабирование по линиям влияет на скорость? ⭐⭐⭐

## Базовые абстракции ОС: процессы, адресные пространства, файлы

- Почему в ОС вообще нужны абстракции (процессы/адресные пространства/файлы), и какие проблемы «железа» они скрывают от программиста? ⭐⭐
- Чем «программа» отличается от «процесса», и какие атрибуты превращают код на диске в выполняющийся процесс? ⭐
- Как адресное пространство помогает одновременно в удобстве программирования и в безопасности? ⭐⭐
- Приведите пример: какая одна и та же операция может выглядеть «как работа с файлом», но на самом деле быть работой с устройством или IPC? Объясните идею унификации. ⭐⭐
- Какие последствия для дизайна ОС возникают из того, что процесс и файл — ключевые универсальные абстракции почти во всех ОС? ⭐⭐⭐
- Что именно хранит ОС о процессе в «таблице процессов», и зачем там нужны значения регистров и позиции в файлах? ⭐⭐
- Какие ресурсы (кроме памяти) обычно «прикреплены» к процессу, и что должно случиться с ними при завершении процесса? ⭐⭐
- Опишите, что должно произойти при приостановке и последующем возобновлении процесса, чтобы программа «ничего не заметила». ⭐⭐
- Как вы объясните различие между состоянием «спит», «выполняется», «остановлен», «зомби» с точки зрения ОС и родителя процесса? ⭐⭐
- Почему процессы часто называют «контейнерами выполнения», и в каком смысле это похоже/не похоже на контейнеры уровня Docker? ⭐⭐⭐
- Какие выводы о системе можно сделать по списку процессов `ps aux`: что искать в USER/PID/STAT/COMMAND? ⭐
- Чем отличаются VSZ/VIRT и RSS/RES, и почему «много VIRT» не всегда означает «проблема с памятью»? ⭐⭐
- Как интерпретировать %CPU в `top` на многоядерной машине: когда 100% — это «всё» и когда — «одно ядро»? ⭐⭐
- Что означает load average, и почему он может быть высоким даже при низком %CPU? ⭐⭐⭐
- Как бы вы нашли «подозрительный» процесс, который редко использует CPU, но постоянно держит диск занятым? Какие поля/инструменты помогут? ⭐⭐⭐
- Почему в UNIX-подобных системах естественно возникает дерево процессов? Что даёт модель «родитель–ребёнок»? ⭐⭐
- Сравните: IPC через каналы (pipes), через файлы и через сокеты — в чём различие по модели использования и по стоимости? ⭐⭐⭐
- Что такое сигнал: чем он принципиально отличается от «сообщения» IPC, и почему его сравнивают с прерываниями? ⭐⭐
- Придумайте сценарий, где сигнал — хороший механизм (например, таймер/ошибка), и сценарий, где сигнал — плохой выбор и лучше IPC. ⭐⭐⭐
- Что может пойти не так, если процесс «не готов» принимать сигнал (нет обработчика) — почему это иногда полезно, а иногда опасно? ⭐⭐

## Пользователи, UID/GID и модель прав

- Зачем ОС связывает процесс с UID/GID, и как это влияет на доступ к файлам и процессам других пользователей? ⭐⭐
- Чем отличается «пользователь» от «группы» в практическом управлении доступом? Приведите пример политики доступа. ⭐⭐
- Почему root (администратор) — одновременно полезная и опасная концепция? Какие риски она создаёт? ⭐⭐
- Какую информацию можно извлечь из `/etc/passwd`, и почему наличие записи там ещё не означает возможность интерактивного входа? ⭐⭐⭐
- Опишите типичную модель «минимально необходимых прав» и как она реализуется на практике в UNIX через пользователей/группы/права. ⭐⭐⭐
- В чём принципиальная разница между `su` и `sudo` с точки зрения модели безопасности? ⭐⭐
- Почему `su` считается менее безопасным подходом в командах/организациях, и как это связано с паролями и аудитом? ⭐⭐
- Что именно меняется при `su`: какие части «сессии» (окружение, текущий каталог, права) могут вести себя иначе? ⭐⭐⭐
- Зачем `sudo` обычно просит пароль текущего пользователя, а не root? Как это помогает контролю и расследованиям? ⭐⭐
- Опишите сценарий, когда `sudo` может быть опасен при неверной настройке (например, чрезмерные права), и как это предотвратить. ⭐⭐⭐

## Адресное пространство и аппаратная защита

- Что такое адресное пространство процесса, и почему «несколько процессов в памяти» требуют аппаратной поддержки? ⭐⭐
- Как процессор и ОС вместе обеспечивают, что процесс не может читать/писать память другого процесса? (Опишите на уровне идеи.) ⭐⭐⭐
- Что такое режим пользователя и режим ядра, и почему системные вызовы требуют перехода между ними? ⭐⭐
- Почему ранние системы могли обходиться без защиты памяти, и какие компромиссы это накладывало на надёжность? ⭐⭐
- Почему во встраиваемых системах защита памяти иногда отсутствует и сегодня: когда это оправдано, а когда — нет? ⭐⭐⭐
- Объясните идею виртуальной памяти: какие иллюзии она создаёт для процесса и какие задачи решает для ОС? ⭐⭐
- Что такое swap и в каких случаях его использование помогает, а в каких — «убивает» производительность? ⭐⭐
- Как связаны «страницы памяти», подкачка и то, что программа может адресовать больше, чем физическая RAM? ⭐⭐⭐
- Как бы вы интерпретировали вывод `free -h` и `vmstat`: какие поля укажут на активную подкачку и дефицит памяти? ⭐⭐⭐
- Почему виртуальная память упрощает код прикладных программ, а не только «даёт больше памяти»? Приведите пример. ⭐⭐⭐

## Файловые системы: пути, каталоги, операции

- Почему файловая система считается ключевой абстракцией ОС, и какие детали устройств она скрывает? ⭐⭐
- Объясните разницу между абсолютным и относительным путём и роль текущего рабочего каталога процесса. ⭐
- Как устроена иерархия каталогов как «дерево», и какие преимущества даёт по сравнению с одноуровневой директорией? ⭐⭐
- Почему операции `open/read/write/close` считаются «минимальным набором» для работы с данными? Что строится поверх них? ⭐⭐
- Чем отличается удаление файла от удаления каталога (`rm` vs `rmdir`), и почему каталог нельзя удалить, пока он не пуст? ⭐⭐
- Что такое монтирование и почему модель «единое дерево» отличается от «букв дисков» в Windows? ⭐⭐
- Объясните роль `/dev` и смысл «специальных файлов»: что это даёт ОС и приложениям? ⭐⭐
- Что такое pipe в терминах потока данных: почему он выглядит как файл, но ведёт себя иначе? ⭐⭐
- Зачем существуют `/dev/stdout` и `/dev/null`: какие практические сценарии они упрощают? ⭐

## Базовые понятия: процесс, программа, абстракция ОС, параллелизм

- Что такое **процесс** с точки зрения операционной системы, и какие компоненты состояния отличают его от «просто программы на диске»? ⭐
- Объясните разницу между понятиями **программа** и **процесс** на примере одного исполняемого файла, запущенного несколько раз. ⭐
- Почему процесс называют **ключевой абстракцией ОС**, и какие механизмы ОС вокруг него построены? ⭐⭐
- Что означает идея **«виртуального CPU»** для процесса, и за счёт чего ОС создаёт эту иллюзию? ⭐⭐
- Какие ресурсы процесса можно считать **логическими**, а какие — **физическими**, и как ОС разделяет эти уровни? ⭐⭐⭐
- Какие свойства процесса делают его удобной единицей **изоляции и безопасности** в системе? ⭐⭐
- В каких случаях процессы в системе **не являются пользовательскими программами**, а служат инфраструктурой ОС? ⭐⭐
- Объясните, что такое **псевдопараллелизм** на одном ядре и почему пользователь воспринимает его как «одновременность». ⭐
- Чем принципиально отличается псевдопараллелизм от **реального параллелизма** на многоядерной системе? ⭐
- Почему в многозадачной системе **скорость выполнения** отдельного процесса становится плохо предсказуемой? ⭐⭐
- Почему **busy-wait / idle loops** — плохой способ тайминга в ОС с вытесняющей многозадачностью? ⭐⭐
- Какие механизмы нужны системе, чтобы поддерживать задачи **реального времени**, и почему «обычное планирование» часто не подходит? ⭐⭐⭐
- Как мультипрограммирование связано с тем, что процессы часто находятся в состоянии **ожидания I/O**? ⭐⭐

## Создание процессов: UNIX fork/exec и Windows CreateProcess

- Перечислите основные ситуации, когда в системе **создаются процессы** (boot, пользователь, родитель, batch) и объясните их отличия. ⭐
- Почему в UNIX исторически сложилась двухшаговая модель **fork() → exec()**, и какие преимущества она даёт? ⭐⭐
- Что именно копирует fork() и что **не копируется**? ⭐⭐
- Что такое **copy-on-write** и почему он делает fork эффективным в реальных ОС? ⭐⭐⭐
- В чём смысл «окна» между fork() и exec() для shell-подобных программ? Приведите примеры действий, которые выполняются именно там. ⭐⭐
- Что делает execve()/execvp() на уровне процесса, и почему говорят «заменяет образ процесса»? ⭐⭐
- Почему в Windows используют **CreateProcess** вместо fork/exec, и какие возможности он даёт сразу при запуске? ⭐⭐
- Какие типичные ошибки делают при работе с CreateProcess (например, командная строка, наследование дескрипторов, ожидание завершения)? ⭐⭐⭐
- Объясните, зачем родителю ждать дочерний процесс (waitpid / WaitForSingleObject), и что может случиться, если этого не делать. ⭐⭐
- Какие бывают причины завершения процесса и чем отличается «ошибка программы» от «принудительного убийства»? ⭐
- Чем отличается **exit-код** процесса от завершения **сигналом**, и почему ОС должна уметь различать эти сценарии? ⭐⭐
- Как родитель в UNIX может определить: ребёнок завершился нормально или был убит сигналом? ⭐⭐
- Что означает «завершение родителя ≠ завершение детей» и какие последствия это имеет для архитектуры сервисов? ⭐⭐
- Почему в многопользовательской системе нельзя разрешать «кому угодно» завершать чужие процессы? Какие механизмы авторизации обычно применяются? ⭐⭐⭐
- Почему fork в C++ может «ломать» привычный жизненный цикл объектов, и какие классы проблем это порождает? ⭐⭐
- Объясните разницу между `exit()` и `_exit()` в дочернем процессе после fork, и почему неправильный выбор может приводить к багам. ⭐⭐⭐
- Приведите примеры ресурсов (файлы, сокеты, lock-файлы, логи), которые после fork могут привести к ошибкам, и предложите стратегии безопасного дизайна. ⭐⭐⭐

## Идентификаторы и дескрипторы, управление процессами

- Что такое PID и какие свойства у него есть (уникальность, переиспользование, область видимости)? ⭐
- В чём отличие PID от HANDLE: почему PID — это «паспорт», а HANDLE — «ключ»? ⭐⭐
- Какие операции обычно можно выполнить, имея только PID, и какие требуют более «сильной ссылки» на объект? ⭐⭐
- Почему в Windows объект ядра может продолжать существовать после завершения процесса, и что определяет момент его уничтожения? ⭐⭐⭐
- Объясните, как в Linux соотносятся PID, TID и TGID, и почему главный поток имеет PID = TID. ⭐⭐
- Как формируется **дерево процессов** в UNIX, и почему процесс PID 1 играет особую роль? ⭐⭐
- Что такое процесс-группа и зачем она нужна в терминальной работе (например, Ctrl-C)? ⭐⭐
- Почему говорят, что Windows «не хранит дерево процессов так же жёстко», как UNIX, и какие практические последствия у этого есть? ⭐⭐
- Что такое «потеря иерархии» при передаче HANDLE другому процессу в Windows, и почему это меняет модель управления? ⭐⭐⭐
- Опишите модель состояний процесса: **Running / Ready / Blocked**. Что означает каждое состояние на практике? ⭐
- Разберите переходы между состояниями: какие из них вызваны **внешними событиями**, а какие — решениями **планировщика**? ⭐⭐
- Почему процесс может быть Ready, но не Running, и какие факторы влияют на то, когда он получит CPU? ⭐⭐
- На примере pipeline `cat | grep` объясните, почему один процесс может быть Blocked, а другой — Running/Ready, и как это связано с I/O. ⭐⭐
- Чем отличается блокировка на I/O от «просто ожидания кванта времени» с точки зрения эффективности системы? ⭐⭐⭐
- Что такое **PCB (Process Control Block)** и какие данные в нём критичны для переключения контекста? ⭐⭐
- Почему таблица процессов — это не просто список, а центральная структура для планировщика, сигналов, памяти и файлов? ⭐⭐⭐
- Опишите, что происходит при **переключении контекста**: какие части состояния сохраняются и где именно. ⭐⭐⭐
- Почему процесс может быть прерван тысячи раз, но «не замечать этого»? Какие условия должны соблюдаться для этой иллюзии? ⭐⭐
- Как связаны понятия **прерывание**, **обработчик**, **вектор прерываний** и последующее решение планировщика? ⭐⭐⭐
- Что такое `task_struct` в Linux и почему его считают «сердцем» модели процессов? ⭐⭐⭐

## Классическая модель потока

- Чем поток принципиально отличается от процесса с точки зрения модели ОС и программиста? ⭐
- Почему потоки называют «процессом внутри процесса» — что именно “внутри”, а что “общее”? ⭐⭐
- Какие типы задач выигрывают от потоков, а какие — почти не выигрывают? Приведите примеры. ⭐⭐
- Объясните идею перекрытия I/O и вычислений на примере: что означает «CPU не простаивает»? ⭐⭐
- Почему создание/уничтожение потоков обычно дешевле, чем процессов? За счёт каких ресурсов/операций? ⭐⭐
- В каких случаях многопоточность может ухудшить производительность по сравнению с однопоточной программой? ⭐⭐⭐
- Какие сущности относятся к «ресурсам процесса», а какие — к «контексту исполнения потока»? ⭐
- Почему процесс называют «единицей управления ресурсами», а поток — «единицей планирования»? ⭐⭐
- Какие ресурсы *обычно* общие для потоков одного процесса, а какие — строго индивидуальные? ⭐⭐
- Что именно хранится в «контексте потока», который требуется для переключения? ⭐⭐
- Как наличие общего адресного пространства упрощает взаимодействие потоков по сравнению с процессами? ⭐
- Почему отсутствие защиты между потоками делает ошибки опаснее, чем при IPC между процессами? ⭐⭐

## Параллелизм и планирование, ресурсы потока

- Чем псевдопараллелизм на одном CPU отличается от реального параллелизма на многоядерной системе? ⭐
- Как ОС решает, какой поток получит CPU следующим (в общих чертах)? ⭐⭐
- Что означают состояния running/ready/blocked/terminated для *потока*, и как они соотносятся с состояниями процессов? ⭐⭐
- Приведите пример перехода thread: running → blocked и объясните, почему это не «ошибка планировщика». ⭐⭐
- В каких ситуациях полезен добровольный yield, и почему он не гарантирует немедленного переключения? ⭐⭐
- Какие метрики/сигналы в системе вы бы смотрели, чтобы понять: приложение ограничено CPU или I/O? ⭐⭐⭐
- Почему у каждого потока должен быть собственный стек? Что сломается при «общем стеке»? ⭐⭐
- Что такое «кадры стека» и почему их структура важна для понимания выполнения потока? ⭐
- Какие данные разделяются потоками в куче/глобальной области, а какие — «живут» на стеке каждого потока? ⭐⭐
- Приведите пример ошибки, когда адрес локальной переменной передают в поток, и объясните причину. ⭐⭐
- Как различается типичная отладка багов «повреждение стека» vs «гонка данных» (на уровне симптомов)? ⭐⭐⭐
- Что означает «ядро не в курсе потоков» в user-level модели, и как библиотека переключает потоки? ⭐⭐
- Почему блокирующий системный вызов (например, read) блокирует **весь процесс** при user-level threads? ⭐⭐
- Почему page fault в user-level модели может «заморозить» все user threads, даже если они логически независимы? ⭐⭐⭐
- Какие обходные решения позволяют делать user-level потоки практичнее (select/poll/epoll, wrapper’ы), и чем они платят? ⭐⭐⭐
- В чём ключевые преимущества kernel threads над user-level threads, и какие накладные расходы они добавляют? ⭐⭐
- Объясните гибридную модель M:N: что планирует ОС, что планирует рантайм, и почему это похоже на goroutines/виртуальные потоки. ⭐⭐⭐

## Жизненный цикл потоков и базовые примитивы

- Опишите типичный жизненный цикл: create → работа → join. Какие ошибки бывают на каждом этапе? ⭐⭐
- Что произойдёт, если не вызвать join/detach для std::thread, и почему стандарт выбрал именно такое поведение? ⭐⭐
- Чем joinable поток отличается от detached (концептуально и по последствиям для ресурсов)? ⭐⭐
- Какие стратегии завершения потоков вы бы применили в сервере: «жёстко убить», «кооперативно остановить», «дождаться» — и почему? ⭐⭐⭐
- Почему важно проектировать «владение задачей» и «владение потоком» отдельно? ⭐⭐⭐
- Объясните назначение pthread_create: какие параметры критичны и почему start_routine имеет сигнатуру void* (void*)? ⭐⭐
- Почему нельзя передавать &i (адрес переменной цикла) как arg в pthread_create? Объясните, когда это «случайно работает». ⭐⭐
- Что делает pthread_join, и какие типичные ошибки приводят к EINVAL/EDEADLK/ESRCH? ⭐⭐⭐
- Зачем существуют pthread_attr_* и какие атрибуты вы бы реально настраивали в практике? ⭐⭐
- Что такое размер стека потока, чем опасен слишком маленький стек, и почему есть PTHREAD_STACK_MIN? ⭐⭐
- Когда вы бы использовали sched_yield, и почему он не является «средством синхронизации»? ⭐⭐
- Почему для программ с C/C++ runtime в Windows рекомендуют _beginthreadex, а не CreateThread? ⭐⭐
- Какие проблемы могут проявиться при CreateThread в программе, активно использующей CRT (printf/malloc/iostream), и почему? ⭐⭐⭐
- Зачем CloseHandle после завершения потока и почему его не делает _endthreadex? ⭐⭐
- Чем отличается HANDLE от thread id (TID) с точки зрения управления потоком? ⭐⭐
- Как бы вы организовали ожидание нескольких потоков в Windows и какие ограничения/нюансы у WaitForMultipleObjects? ⭐⭐⭐
- Сравните std::thread и std::jthread: какие риски снижает jthread и какой ценой? ⭐⭐
- Почему «RAII для потока» в виде std::jthread — важная идея для надёжности? ⭐⭐
- Объясните идею кооперативной отмены через std::stop_token: что должно делать тело потока? ⭐⭐
- Что произойдёт в примере с SortVector, если **не сохранять** возвращаемый std::jthread в переменную? Почему? ⭐⭐⭐

  ```cpp
  template <typename T>
  std::jthread SortVector(std::vector<T>& values) {
    return std::jthread{ [&values] {
      std::ranges::sort(values);
    } };
  }

  int main()
  {
    std::vector<int> numbers{ 10, 2, -5, 3, 17, 5 };
    std::vector<std::string> strings{ "one", "two", "three", "four", "five" };
    {
      auto t1 = SortVector(numbers); // Что если не сохранять результат в переменную?
      auto t2 = SortVector(strings);
    }
  }
  ```

- В каких случаях вы бы предпочли std::thread вместо std::jthread? ⭐⭐⭐

## Базовые понятия IPC и синхронизации

- Чем принципиально отличаются **синхронизация** и **коммуникация** между процессами/потоками?
   Приведите примеры, где нужна только одна из них, и где нужны обе. ⭐⭐
- Почему общая память «упрощает обмен данными», но одновременно «усложняет синхронизацию»? Разберите на примере инварианта структуры данных. ⭐⭐
- Дайте определения: **race condition**, **data race**, **atomicity violation**. Чем они отличаются и как проявляются? ⭐⭐⭐
- Что такое **критическая секция**? Какие свойства кода/ресурса делают участок «критическим»? ⭐
- Какие 4 условия корректного решения задачи критической секции (mutual exclusion) вы считаете ключевыми, и почему каждое из них важно на практике? ⭐⭐
- Почему пример с `counter++/counter--` на двух потоках может дать «случайный» результат, хотя на вид код симметричен? Опишите возможные межпоточные интерливинги. ⭐⭐

  ```cpp
  void Increment(int& counter) { ++counter; }
  void Decrement(int& counter) { --counter; }
  
  int main() {
    int counter = 0;
    std::jthread t1{ [&counter] {
    for (int i = 0; i < 1'000'000'000; ++i)
      Increment(counter);
    } };
    std::jthread t2{ [&counter] {
    for (int i = 0; i < 1'000'000'000; ++i)
      Decrement(counter);
    } };
    t1.join(); t2.join();
    std::cout << "Counter: " << counter << '\n';
  }
  ```

- Объясните, как исправить проблему в коде выше. ⭐⭐
- Что означает префикс `lock` в x86-инструкциях (например, `lock add`)? Как это связано с когерентностью кэша и «атомарностью»? ⭐⭐⭐
- Почему `volatile` **не является** механизмом синхронизации в C++? Приведите пример, где `volatile` не спасает от гонки. ⭐⭐⭐
- Объясните смысл **acquire/release** на примере «данные + флаг готовности». Что такое *happens-before*? ⭐⭐⭐

## Классические алгоритмы синхронизации

- Почему **busy waiting** считается проблемой в пользовательских программах? Назовите минимум 3 причины. ⭐
- Почему «запрет прерываний» работает как механизм взаимного исключения на одноядерной системе, и почему ломается на SMP? ⭐⭐
- Чем отличается «запрет прерываний» от «атомарной инструкции с блокировкой шины/кэша» с точки зрения охвата других CPU? ⭐⭐
- Почему инструкции `CLI/STI` недоступны в ring 3 и что произойдёт при попытке выполнить их в user mode? ⭐⭐
- В каких случаях ядро ОС всё же использует отключение прерываний, и почему время удержания должно быть «считанные инструкции»? ⭐⭐⭐
- Почему «простая lock-переменная» не обеспечивает взаимного исключения? Опишите гонку пошагово. ⭐
- Почему «прочитать lock дважды» не решает проблему? Сформулируйте, какая именно гарантия отсутствует. ⭐
- Что такое **Strict Alternation** и в чём его ключевой дефект с точки зрения требований к критической секции? ⭐⭐
- Приведите сценарий, где Strict Alternation блокирует процесс, хотя критическая секция свободна. Почему это плохо для производительности? ⭐⭐
- Можно ли «починить» Strict Alternation без аппаратной атомарности? Если да — какой ценой; если нет — почему? ⭐⭐⭐
- Объясните идею алгоритма Петерсона: зачем нужны **interested[]** и **turn** одновременно? ⭐⭐
- Докажите (словами) хотя бы одно свойство: mutual exclusion / progress / bounded waiting для алгоритма Петерсона. ⭐⭐⭐
- Почему реализация Петерсона на обычных `bool/int` может ломаться на современных CPU? ⭐⭐⭐
- Почему добавление `volatile` всё равно может не исправить Петерсона в C++? Что именно `volatile` гарантирует и чего не гарантирует? ⭐⭐⭐
- Как корректно реализовать Петерсона в C++ с `std::atomic`? Какие memory order’ы уместны и почему? ⭐⭐⭐

## Спинлоки

- Что делает TSL (test-and-set) и почему эта операция считается атомарной? ⭐⭐
- Сравните TSL и XCHG как примитивы: в чём концептуальная одинаковость и какие есть архитектурные нюансы. ⭐⭐
- Почему спинлоки эффективны только для «очень коротких» критических секций? Опишите критерии «короткости». ⭐⭐
- Что такое **starvation** в контексте спинлоков и почему отсутствие fairness — нормальная цена за простоту? ⭐⭐
- Какие техники применяют, чтобы уменьшить вред спина под конкуренцией (pause/yield/backoff)? Почему это помогает? ⭐⭐⭐
- Объясните, как `atomic_flag::test_and_set` реализует идею TSL. Что возвращает и почему это удобно для лока? ⭐⭐
- Почему в примере используются `memory_order_acquire` на lock и `memory_order_release` на unlock? Что будет, если поставить `relaxed`? ⭐⭐⭐
- Какие практические проблемы у такого спинлока: рекурсивность, fairness, влияние на энергопотребление, масштабирование на много ядер? ⭐⭐
- Почему внутри `std::atomic_flag` реализация может быть сложнее, чем кажется? Как это влияет на производительность? ⭐⭐⭐
- Как бы вы добавили в TSLLock `try_lock()` и «вежливое ожидание»? Какие компромиссы появятся? ⭐⭐

## Спинлоки в ядре: IRQ и вытеснение

- Зачем ядру нужно различать `spin_lock`, `spin_lock_irq` и `spin_lock_irqsave`? Опишите типичный сценарий для каждого. ⭐⭐⭐
- Почему «спать» внутри спинлока нельзя? Приведите пример deadlock-сценария. ⭐⭐
- В чём опасность, если обработчик прерывания попытается захватить тот же спинлок на том же CPU? Как это предотвращают? ⭐⭐⭐
- Почему `spin_lock_irq` может быть быстрее `irqsave`, но потенциально опаснее? ⭐⭐
- Что происходит с латентностью прерываний, если слишком часто/долго держать spin_lock_irqsave? Чем это грозит системе? ⭐⭐⭐
- В чём фундаментальная проблема примитивов `sleep()`/`wakeup()` как абстракции, если нет «памяти о сигнале»? ⭐⭐
- Разберите сценарий «потерянного wakeup» в producer-consumer из слайдов: какие именно шаги приводят к вечному сну? ⭐⭐⭐
- Почему «wakeup waiting bit» решает проблему только для двух процессов и начинает ломаться при увеличении числа участников? ⭐⭐⭐
- Как связаны sleep/wakeup, очереди ожидания и «ожидание по адресу» (sleep(address)/wakeup(address))? Почему привязка к адресу полезна? ⭐⭐
- Чем отличаются «sleep как задержка по времени» (nanosleep) и «sleep как IPC/синхронизация»? Почему путаница опасна? ⭐⭐
- Семафор как «счётчик сохранённых wakeup’ов»: объясните модель и почему она предотвращает потерю сигналов. ⭐⭐
- Почему операции `down/up` в семафоре должны быть атомарными? Что именно должно быть «неделимо» и какая гонка иначе появится? ⭐⭐⭐
- В задаче producer-consumer объясните роль трёх семафоров `mutex/empty/full` как **двух разных применений**: mutual exclusion vs ordering. ⭐⭐
- Сравните `std::counting_semaphore` и `std::condition_variable` как инструменты синхронизации: где проще семафор, а где — condvar? ⭐⭐⭐
- Как бы вы спроектировали ограничитель ресурсов (например, «не больше K одновременных запросов») в C++20: какие примитивы выберете и какие corner cases учтёте? ⭐⭐⭐

## Мьютексы: идея и реализация

- Почему мьютекс называют «упрощённым семафором»? В каких задачах семафор принципиально сильнее мьютекса? ⭐⭐
- Какие ошибки синхронизации мьютекс предотвращает, а какие **не предотвращает** (например, логические гонки и нарушение инвариантов)? ⭐⭐
- Объясните, почему «достаточно 1 бита», но на практике состояние мьютекса хранится как целое число/слово. ⭐⭐
- Разберите реализацию `mutex_lock` через TSL: что происходит при конкуренции, и почему используется `thread_yield()`? ⭐⭐
- Чем подход `mutex_lock + yield` отличается от чистого спинлока? Когда yield ухудшает ситуацию? ⭐⭐⭐
- Зачем нужен `trylock()`? Приведите пример алгоритма, где `trylock` позволяет избежать дедлока или улучшить latency. ⭐⭐
- Почему мьютексы «естественно» работают для потоков, но требуют дополнительных механизмов для процессов? ⭐⭐
- Какие способы межпроцессного мьютекса возможны: ядровые объекты, shared memory, файлы? Какие у каждого плюсы/минусы? ⭐⭐⭐
- Какие свойства должен иметь мьютекс, чтобы быть безопасным для межпроцессной синхронизации? ⭐⭐⭐
- Почему «мьютекс в user space» обычно быстрее, чем ядровая блокировка, и в каких случаях это перестаёт быть правдой? ⭐⭐⭐

## Futex и condition variables

- Какую проблему выбора «spinlock vs блокировка через ядро» решает futex? ⭐⭐
- Объясните, что означает «в отсутствии конкуренции ядро не вовлекается вообще». За счёт чего это достигается? ⭐⭐⭐
- Опишите жизненный цикл захвата futex: атомарная попытка → системный вызов → очередь ожидания → пробуждение. ⭐⭐⭐
- Почему futex редко используется напрямую прикладным программистом, но часто встречается «под капотом»? ⭐⭐
- Какие ошибки могут возникнуть при неправильном использовании futex (например, потерянные пробуждения, ABA-сценарии, неверные ожидания)? ⭐⭐⭐
- Какие гарантии даёт `pthread_mutex_lock` и что он НЕ гарантирует (например, порядок пробуждений/справедливость)? ⭐⭐
- Почему условные переменные всегда используются **вместе с мьютексом**? ⭐⭐
- Что означает фраза «условные переменные не накапливают сигналы» и чем это отличается от семафоров? ⭐⭐⭐
- Объясните, почему `pthread_cond_wait(&cond, &mutex)` атомарно «отпускает мьютекс и засыпает». Почему это важно? ⭐⭐⭐
- В producer/consumer примере почему проверка условия должна быть в `while`, а не `if`? Назовите минимум две причины. ⭐⭐
- Чем отличаются `std::lock_guard` и `std::unique_lock`? Почему `wait()` требует именно `unique_lock`? ⭐⭐
- Что такое spurious wakeup и почему он возможен даже при корректной логике notify? ⭐⭐⭐
- В чём разница между `cv.wait(lock)` и `cv.wait(lock, predicate)` с точки зрения безопасности и читаемости? ⭐⭐
- Какую типичную ошибку делают при использовании `notify_one()`/`notify_all()` (например, уведомление без изменения состояния)? ⭐⭐
- Опишите корректный дизайн очереди producer-consumer на C++: где хранится условие, где мьютекс, где уведомление, что защищаем. ⭐⭐⭐

## std::mutex и разновидности мьютексов

- В каких случаях оправдан `std::recursive_mutex`, и почему его часто считают «запахом дизайна»? ⭐⭐⭐
- Чем полезны `std::timed_mutex` и таймауты в синхронизации? Приведите пример, где таймаут — часть корректности. ⭐⭐⭐
- Почему RAII-обёртки (lock_guard/unique_lock) уменьшают вероятность дедлоков и утечек блокировки? ⭐⭐
- Какие риски возникают, если внутри критической секции делать I/O или ждать другие ресурсы? ⭐⭐
- Какие способы избежать дедлоков при захвате нескольких мьютексов (lock ordering, std::lock, try_lock loop)? ⭐⭐⭐
- Чем `std::shared_mutex` принципиально отличается от обычного `std::mutex` по модели допуска потоков? ⭐⭐
- В каких сценариях `std::shared_mutex` реально ускоряет систему, а в каких может замедлить? ⭐⭐
- Почему стандарт не гарантирует справедливость в `std::shared_mutex` и к чему это приводит для писателей? ⭐⭐⭐
- Почему upgrade (shared → unique) небезопасен без выхода? Опишите гонку, которая возникает при «наивном апгрейде». ⭐⭐⭐
- Почему `shared_mutex` подходит только для коротких критических секций, несмотря на «параллельность чтения»? ⭐⭐

## FairRWLock и мониторы

- В чём идея writer-preferred RWLock и как она предотвращает starvation писателей? ⭐⭐
- Какие поля нужны FairRWLock (active readers / waiting writers / active writer) и какие инварианты они должны соблюдать? ⭐⭐⭐
- Почему в `lock_shared()` читатели должны ждать не только `active_writer == false`, но и `waiting_writers == 0`? ⭐⭐⭐
- Объясните логику пробуждений: почему в `unlock()` иногда будим писателя, а иногда всех читателей? ⭐⭐⭐
- Какие типичные ошибки приводят к дедлоку или «зависанию» в RWLock (например, неверный notify, неправильные условия ожидания)? ⭐⭐⭐
- Почему мониторы появились как реакция на «опасность семафоров»? Какие классы ошибок они уменьшают? ⭐⭐
- Что означает «только один поток активен в мониторе» и как это реализуется на практике (на уровне языка/рантайма)? ⭐⭐
- Сравните семантику сигнализации Хоара и Бринч Хансена: кто продолжает выполнение после `signal()` и почему это важно. ⭐⭐⭐
- Почему даже в мониторах сигналы condition variables «не копятся»? Какие последствия это имеет для дизайна? ⭐⭐⭐
- Как модель монитора в Java (`synchronized`, `wait/notify`) отличается от «классического» монитора с отдельными condition variables? ⭐⭐⭐
- Какие проблемы появляются при IPC через сообщения в распределённой среде: потери, дубликаты, порядок, подмена отправителя? ⭐⭐
- Зачем нужны ACK + ретрансляции и почему это сразу приводит к необходимости sequence numbers? ⭐⭐⭐
- Чем отличается адресация «процесс → процесс» от mailboxes? Как mailboxes помогают масштабировать систему? ⭐⭐
- Сравните rendezvous (без буфера) и буферизированный обмен: какие плюсы/минусы по latency, throughput и сложности? ⭐⭐⭐
- Объясните producer–consumer без общей памяти через «пустые/полные» сообщения: какой инвариант делает систему корректной? ⭐⭐⭐

## Барьеры синхронизации (phase synchronization), ошибки работы с многопоточностью

- Объясните, чем барьер отличается от мьютекса/семафора: какую задачу он решает и какую — нет? ⭐⭐
- В каких типах алгоритмов барьер является «естественным» примитивом, и почему там нельзя просто поставить мьютекс? ⭐⭐
- Что означает «никто не переходит к фазе n+1, пока все не закончили фазу n» в терминах корректности данных между итерациями? ⭐⭐
- Как устроен переиспользуемый барьер: зачем нужны счётчик участников и «поколение»? ⭐⭐⭐
- Объясните, какую роль играет `completion step` в `std::barrier`, и приведите пример, где без него легко ошибиться. ⭐⭐⭐
- Какие ошибки приводят к дедлоку на барьере, и какие стратегии защиты применяют промышленные реализации? ⭐⭐⭐
- Что такое «straggler» и почему он может полностью «убить» производительность параллельного цикла, даже если барьер реализован идеально? ⭐⭐
- Объясните паттерн «current/next + swap на барьере»: какие гонки он предотвращает и почему это лучше, чем писать в один и тот же массив. ⭐⭐⭐

  ```cpp
  std::barrier sync(N, [&]{
    std::swap(current, next);
  });

  void worker(chunk c) {
    for (int it=0; it<steps; ++it) {
      compute(next, current, c);  // читаем из current, пишем в next
      sync.arrive_and_wait();
    }
  }
  
  ```

- В каких ситуациях двойная буферизация не спасает и что тогда делают? ⭐⭐⭐
- Какие инварианты должны выполняться на границе итерации, чтобы следующий шаг не увидел «смешанные» данные? ⭐⭐
- Почему нельзя путать синхронизационный барьер (group barrier) и memory fence? Приведите пример, где один нужен, а другой — нет. ⭐⭐⭐
- Опишите сценарий, когда из-за out-of-order execution «флаг готовности» становится видимым раньше данных, и к чему это приводит. ⭐⭐
- Объясните, как пара `store(..., release)` и `load(..., acquire)` обеспечивает корректную публикацию данных (happens-before). ⭐⭐⭐
- В примере с `turn` и `x`: почему `x` можно читать `relaxed`, но всё равно гарантированно увидеть 100 после `acquire` на `turn`? ⭐⭐⭐
  
  ```cpp
  std::atomic<int> turn{0};
  std::atomic<int> x{0};
  
  // writer:
  x.store(100, std::memory_order_relaxed);
  turn.store(1, std::memory_order_release); // публикуем флаг ПОСЛЕ данных

  // reader:
  while (turn.load(std::memory_order_acquire) != 1) { /* spin */ }
  int v = x.load(std::memory_order_relaxed); // видим 100 гарантированно  
  ```

- В каких случаях уместно использовать `std::atomic_thread_fence`, а когда лучше выразить зависимость через acquire/release на конкретных атомиках? ⭐⭐⭐
- Разберите по шагам механизм инверсии приоритетов с потоками L/M/H: почему наличие «среднего» приоритета делает ситуацию хуже? ⭐⭐⭐
- Сравните Priority Inheritance и Priority Ceiling: что они гарантируют, какие вводят накладные расходы и какие риски/сложности (например, цепочки наследования или настройка потолков)? ⭐⭐⭐
- Почему «отключить прерывания» — плохое решение для пользовательского кода и непереносимая стратегия для общей синхронизации? ⭐⭐

## Планирование процессов, контекст планирования

- Объясните, почему в многопрограммной системе вообще возникает необходимость планирования CPU, и чем «ready state» отличается от «running» и «blocked». ⭐
- В чём разница между планировщиком и алгоритмом планирования? Приведите примеры решений, которые принимает планировщик. ⭐
- Почему принципы планирования применимы не только к процессам, но и к потокам? Что меняется, когда планируем потоки вместо процессов? ⭐⭐
- Сравните роль планирования в пакетных системах, на персональных компьютерах и на серверах. Почему важность планирования так различается? ⭐⭐
- Приведите примеры сценариев, где планирование становится критичным даже на «быстром» компьютере (рендеринг, игры и т. п.). Что именно в нагрузке делает планирование важным? ⭐⭐
- Почему в IoT/смартфонах планирование тесно связано с энергопотреблением? Какие компромиссы при этом возникают? ⭐⭐
- Опишите, что происходит при context switch на уровне CPU и ОС (режим ядра, регистры, память, кэши). Почему это «дорого»? ⭐⭐
- Какие компоненты накладных расходов переключения контекста зависят от архитектуры памяти (MMU, TLB) и кэширования? ⭐⭐
- Почему слишком частые переключения процессов могут снижать реальную полезную производительность системы даже при высокой загрузке CPU? ⭐⭐
- Как выбор длины кванта (time slice) влияет на долю времени, теряемую на переключения? Объясните на качественном уровне. ⭐⭐
- Какие метрики или симптомы в системе могут указывать, что накладные расходы на переключения стали чрезмерными? ⭐⭐⭐

## Виды процессов и их планирование

- Дайте определение CPU burst и I/O burst. Почему ключевым фактором для классификации CPU-bound/I/O-bound считается длина CPU burst? ⭐
- Приведите примеры типичных CPU-bound и I/O-bound задач в современных ОС и объясните их поведение через bursts. ⭐⭐
- Почему ускорение CPU в исторической перспективе «делает процессы более I/O-bound»? Какие технологические причины лежат в основе? ⭐⭐
- Объясните, почему для производительности системы важно «смешивать» CPU-bound и I/O-bound процессы (баланс CPU и устройств). ⭐⭐
- Что будет происходить с загрузкой CPU и диска, если в памяти окажутся только CPU-bound задачи, а затем только I/O-bound? Почему это плохо? ⭐⭐
- Как выбор алгоритма планирования может улучшать «параллелизм» CPU и I/O в системе? Приведите пример стратегии для I/O-bound процессов. ⭐⭐⭐
- Перечислите основные события, в которых ОС может принять решение о перепланировании (создание, завершение, блокировка, I/O interrupt, таймер). Почему этих событий достаточно? ⭐⭐
- В чём принципиальная разница между preemptive и nonpreemptive планированием? Какие риски есть у каждого подхода? ⭐⭐
- Почему наличие периодических прерываний таймера критично для вытесняющего планирования? Что произойдёт, если таймера нет? ⭐⭐
- Объясните, почему вытесняющая многозадачность важна не только для приложений, но и для ядра (preemptive kernel). ⭐⭐
- Приведите пример ситуации, когда невытесняющее планирование может выглядеть привлекательным, и объясните, чем это может обернуться в реальной системе. ⭐⭐⭐

## Категории систем и цели планирования

- Сравните ключевые цели планирования для batch, interactive и real-time систем. Почему одна и та же метрика не подходит всем? ⭐⭐
- Что такое fairness в планировании? Как «справедливость» может конфликтовать с производительностью или политикой приоритетов? ⭐⭐⭐
- Объясните разницу между throughput и turnaround time. Почему максимизация throughput может ухудшать turnaround? ⭐⭐
- Почему CPU utilization — спорная метрика качества планирования в batch-системах? Когда она всё же полезна? ⭐⭐
- Что такое response time в интерактивных системах и почему «пропорциональность ожиданиям пользователя» важна как отдельная цель? ⭐⭐
- Почему в real-time системах «правильный ответ слишком поздно» может быть эквивалентен ошибке? Приведите примеры. ⭐⭐
- Чем отличается predictability от просто «высокой скорости» в real-time/мультимедиа? Почему jitter критичен? ⭐⭐⭐
- Опишите алгоритм FCFS и объясните, почему его называют «справедливым», но при этом он может давать плохую среднюю производительность. ⭐⭐
- Что такое «эффект конвоя» в FCFS? Опишите сценарий с CPU-bound и множеством I/O-bound процессов и последствия для времени завершения. ⭐⭐⭐
- Объясните идею SJF (Shortest Job First) и почему он минимизирует среднее turnaround time при известной длительности задач. ⭐⭐
- В каких условиях SJF является оптимальным, а в каких — нет? Приведите контрпример с разными временами прихода задач. ⭐⭐⭐
- Чем SRTN (Shortest Remaining Time Next) отличается от SJF и почему вытеснение помогает «коротким» задачам? ⭐⭐
- Какие практические проблемы возникают при попытке применить SJF/SRTN в реальных ОС (оценка времени, ошибки прогнозов, starvation)? ⭐⭐⭐

## Интерактивные системы: Round Robin и приоритеты

- Опишите работу Round Robin и объясните роль кванта времени. Почему RR считают «простым и справедливым»? ⭐⭐
- Как слишком короткий квант влияет на CPU efficiency, а слишком длинный — на response time? Почему диапазон 20–50 мс часто компромиссный? ⭐⭐
- В каких ситуациях RR фактически становится почти невытесняющим? Свяжите это со средним CPU burst. ⭐⭐⭐
- Что такое планирование по приоритетам? Какие реальные причины заставляют ОС вводить приоритеты? ⭐⭐
- Почему приоритетные схемы могут приводить к starvation? Какие механизмы борьбы с этим используются (aging, динамика приоритета и т. п.)? ⭐⭐⭐
- Объясните идею динамического повышения приоритета для I/O-bound процессов. Почему это улучшает общую производительность системы? ⭐⭐⭐
- Почему удобно сочетать приоритеты «между классами» и round robin «внутри класса»? Какие плюсы и минусы у такого гибрида? ⭐⭐
- Опишите идею multiple queues (многоуровневых очередей) и объясните, как они помогают одновременно интерактивным и вычислительным задачам. ⭐⭐
- Почему увеличение кванта на нижних уровнях (1, 2, 4, 8, …) уменьшает число дорогих переключений/свапов в некоторых системах? ⭐⭐⭐
- Приведите пример «обхода» политики многоуровневых очередей пользователем (например, искусственно имитировать интерактивность). Почему «правильно в теории» сложно реализовать на практике? ⭐⭐⭐
- Как можно адаптировать идею SJF к интерактивным системам, если длительности заранее неизвестны? Опишите подход с прогнозированием. ⭐⭐
- Объясните формулу экспоненциального сглаживания (aging) для оценки следующего CPU burst и смысл параметра (a). ⭐⭐⭐
- В чём идея guaranteed scheduling (примерно 1/n CPU каждому) и как измерение «получено vs положено» влияет на выбор следующего процесса? ⭐⭐
- Объясните основную идею Linux CFS на концептуальном уровне: что такое «spent execution time», зачем дерево, почему выбирают «самого недополучившего». ⭐⭐⭐
- Сравните lottery scheduling и fair-share scheduling: что именно считается «справедливостью» в каждом, и в каких сценариях один подход предпочтительнее другого? ⭐⭐⭐

## Планирование в системах реального времени

- Что делает систему «реального времени» отличной от просто «быстрой» системы? Почему «правильный ответ слишком поздно» может считаться ошибкой? ⭐⭐
- Приведите по 2–3 примера hard real-time и soft real-time систем и объясните, что именно является дедлайном в каждом примере. ⭐⭐
- В чём ключевое различие между hard real-time и soft real-time с точки зрения последствий пропуска дедлайна? ⭐⭐
- Почему для мультимедиа-систем важна не только скорость, но и предсказуемость (jitter)? Как планировщик может влиять на качество аудио/видео? ⭐⭐⭐
- Объясните, почему в некоторых real-time системах вытеснение «иногда не требуется». При каких предпосылках это возможно? ⭐⭐⭐
- Сравните периодические и непериодические события в real-time системах. Чем они отличаются с точки зрения планирования и гарантий? ⭐⭐
- Раскройте смысл условия планируемости ($\sum (C_i/P_i) \le 1$). Что означают ($C_i$) и ($P_i$) физически и как интерпретировать сумму? ⭐⭐
- Почему при ($\sum (C_i/P_i) > 1$) система принципиально не может быть корректно запланирована? Какие стратегии остаются в таком случае? ⭐⭐⭐
- В приведённом примере (Периоды: 100/200/500 мс и время обработки 50/30/100 мс) объясните, почему система считается планируемой? ⭐⭐
- В каких реальных ситуациях допущение «накладные расходы на переключение контекста можно игнорировать» становится неверным и как это влияет на проверку планируемости? ⭐⭐⭐
- Предложите метод, как учитывать накладные расходы (context switch, обработка прерываний) при оценке планируемости, не углубляясь в конкретный алгоритм. ⭐⭐⭐
- Почему добавление непериодических задач усложняет гарантии дедлайнов даже при выполнении условия для периодических задач? ⭐⭐⭐
- В чём разница между static scheduling и dynamic scheduling в real-time системах? Какие сведения нужны «заранее» в статическом варианте? ⭐⭐
- Приведите пример системы/сценария, где статическое планирование предпочтительнее, и объясните, почему. ⭐⭐
- Приведите пример системы/сценария, где динамическое планирование неизбежно, и объясните, какую неопределённость оно покрывает. ⭐⭐
- Какие риски и ограничения появляются при статическом планировании, если «идеальная информация заранее» недоступна или ошибочна? ⭐⭐⭐
- Как бы вы объяснили компромисс «предсказуемость vs гибкость» между static и dynamic scheduling на уровне архитектуры системы? ⭐⭐⭐

## Политика против механизма, user-level потоки

- Дайте определения «механизм» и «политика» применительно к планированию. Почему их разделение считается фундаментальным принципом ОС? ⭐⭐
- На примере СУБД объясните, почему приложение может «лучше знать», как планировать свои дочерние задачи/потоки, чем ядро ОС. ⭐⭐
- Приведите примеры параметров/интерфейсов ОС, через которые пользовательский процесс может влиять на политику, не реализуя механизм (приоритеты, квоты и т. п.). ⭐⭐
- Какие опасности возникают, если приложениям дать слишком много контроля над политикой (например, возможность бесконтрольно повышать приоритет)? Как ОС может это ограничивать? ⭐⭐⭐
- Опишите ситуацию, где отсутствие разделения policy/mechanism приводит к заметной потере производительности или нарушению требований по времени. ⭐⭐⭐
- Почему наличие двух уровней параллелизма (процессы и потоки) усложняет планирование по сравнению с «только процессами»? ⭐⭐
- Объясните, как происходит планирование при user-level threads: кто и что планирует, и почему отсутствует принудительное вытеснение потоков таймером? ⭐⭐
- В чём главный практический недостаток user-level threads при блокирующих операциях ввода-вывода? Как он проявляется для пользователя? ⭐⭐
- Объясните, как планирование отличается для kernel-level threads и почему становится возможна «перемежающаяся» последовательность потоков разных процессов. ⭐⭐
- Почему переключение между kernel-level threads обычно дороже, чем между user-level threads? Какие компоненты ОС делают его дорогим? ⭐⭐⭐
- Почему ядру выгоднее (при прочих равных) продолжать выполнять поток в том же процессе, а не переключаться на поток другого процесса? Свяжите ответ с памятью и кэшами. ⭐⭐⭐
- В чём идея гибридного подхода (сочетание user-level и kernel-level) и какие проблемы каждого уровня он пытается компенсировать? ⭐⭐⭐
- Сформулируйте общий вывод лекции для случая real-time + threads: какие свойства планировщика становятся ключевыми (дедлайны, предсказуемость, накладные расходы, политика/механизм) и почему? ⭐⭐⭐

## Память как ресурс

- Почему оперативная память считается одним из ключевых ресурсов системы, и как её нехватка влияет на работу ОС? ⭐
- Объясните, почему «программы растут быстрее, чем объём памяти». Какие причины этого явления вы видите сегодня? ⭐⭐
- Что означает идея «идеальной памяти», и почему она невозможна на практике? ⭐
- Почему компромисс в виде **иерархии памяти** оказался эффективнее попыток сделать один «идеальный» тип памяти? ⭐⭐
- Какие задачи должна решать ОС, чтобы распределять память между процессами справедливо и эффективно? ⭐⭐
- Сравните кеш, RAM и SSD/HDD по скорости, цене и энергозависимости. Почему именно так устроена иерархия? ⭐
- Что означает фраза «ОС должна создать единое абстрактное представление памяти»? ⭐⭐
- Почему доступ к памяти может быть на порядки медленнее, чем операции над регистрами? ⭐⭐
- Как различия во времени доступа (L1/L2/L3/RAM/NUMA/диск) влияют на проектирование программ и алгоритмов? ⭐⭐⭐
- Приведите пример ситуации, когда оптимизация вычислений почти не помогает, потому что узкое место — память. ⭐⭐
- Что такое **менеджер памяти** в ОС и какие функции он обязан выполнять? ⭐
- Почему учёт свободной/занятой памяти является отдельной сложной задачей даже при большом объёме RAM? ⭐⭐
- Почему управление кешем чаще относится к аппаратуре, а не к ОС? ⭐⭐
- Какие ошибки управления памятью могут приводить к падениям программ и всей системы? Приведите примеры. ⭐⭐
- Почему управление памятью часто рассматривают как баланс между производительностью, экономией и предсказуемостью? ⭐⭐⭐

## Память в ранних системах

- Что означает «программа имеет доступ ко всем адресам физической памяти», и почему это опасно? ⭐
- Почему при прямой адресации «две программы не могут существовать в памяти одновременно»? ⭐⭐
- Объясните, как ошибка в одной программе могла уничтожить данные другой программы или ОС в ранних системах. ⭐⭐
- Почему абсолютные адреса в коде делают программу негибкой при загрузке в другое место памяти? ⭐⭐
- Какие преимущества и недостатки имеют системы без виртуальной памяти (например, во встраиваемых устройствах)? ⭐⭐⭐
- Какие варианты размещения ОС в памяти существовали исторически и почему? ⭐⭐
- Почему размещение ОС в RAM без защиты приводило к риску её перезаписи пользователем? ⭐
- Зачем BIOS размещали в ROM в ранних ПК, и какие задачи он решал? ⭐⭐
- Почему даже размещение части ОС в ROM не решало проблему защиты пользовательской памяти полностью? ⭐⭐
- Какие требования к памяти появляются, когда система переходит от «одна программа в памяти» к многозадачности? ⭐⭐⭐
- В чём идея swapping как раннего способа создать иллюзию многозадачности? ⭐⭐
- Как работали аппаратные ключи защиты памяти в IBM/360 и что именно они предотвращали? ⭐⭐⭐
- Почему механизм ключей защиты решал задачу безопасности, но не решал проблему релокации? ⭐⭐
- Что такое PSW и какую роль он играет в контроле доступа к памяти (на концептуальном уровне)? ⭐⭐
- Какие ограничения у схемы защиты памяти «ключами», если сравнивать с полноценными адресными пространствами? ⭐⭐⭐

## Адресные пространства

- Почему при загрузке программы не с нулевого адреса команды перехода/обращения к данным могут «сломаться»? ⭐⭐
- Что такое **статическая релокация**, и как ОС корректирует адреса при загрузке программы? ⭐⭐
- Почему загрузчику важно различать «адрес» и «константу» в машинном коде? ⭐⭐⭐
- Зачем компилятор формирует таблицу релокаций, и что в ней должно быть? ⭐⭐
- Почему статическая релокация делает загрузку медленной и сложной, особенно при частых перемещениях программ? ⭐⭐⭐
- Зачем нужна абстракция памяти и почему без неё сложно обеспечить безопасность и стабильность? ⭐⭐
- Дайте определение адресного пространства процесса и объясните, почему «адрес 28 в одном процессе ≠ адрес 28 в другом». ⭐⭐
- Приведите аналогии адресного пространства с реальными системами (телефонные номера, IP, домены) и объясните их смысл. ⭐
- Как ОС и аппаратное обеспечение совместно сопоставляют виртуальные адреса с физическими? (общая идея) ⭐⭐⭐
- Какие свойства даёт адресное пространство: изоляция, защита, гибкое размещение — и как каждое проявляется? ⭐⭐⭐

## Защита и релокация, swapping

- Объясните принцип работы **base register** и **limit register** и как они обеспечивают защиту процесса. ⭐⭐
- Почему при base/limit программа может «думать», что начинается с адреса 0, даже если в физической памяти она размещена иначе? ⭐⭐
- Что происходит при попытке обращения за пределы limit, и почему это важно для устойчивости системы? ⭐⭐
- Почему изменять base/limit должна иметь право только ОС? Какие угрозы возникают иначе? ⭐⭐⭐
- Какие недостатки у схемы base/limit с точки зрения производительности и почему они проявляются при каждом обращении к памяти? ⭐⭐⭐
- Почему в современных ОС возникает необходимость выгружать процессы на диск, даже при наличии больших объёмов RAM? ⭐⭐
- Опишите полный цикл swapping: загрузка → выполнение → выгрузка → возврат. Какие данные должны сохраняться? ⭐⭐⭐
- Почему при возврате процесса из swap он может оказаться в другом месте памяти, и что нужно для корректной работы? ⭐⭐
- Что такое «дыры» (holes) в памяти и почему swapping усиливает фрагментацию? ⭐⭐
- Объясните, что такое memory compaction, почему она дорогая, и в каких ситуациях может быть оправдана. ⭐⭐⭐

## Управление памятью с помощью битовых карт

- Почему динамическое выделение памяти превращает «свободную память» в отдельный управляемый ресурс ОС? ⭐⭐
- Какие ошибки и сбои возникают, если ОС неправильно ведёт учёт свободных участков памяти? ⭐⭐
- Что такое «учётное представление памяти» и какие требования к нему предъявляются (скорость, точность, надёжность)? ⭐⭐
- Почему задачи учёта свободной памяти похожи на учёт свободных блоков диска в файловой системе? ⭐⭐
- Чем отличаются цели управления памятью **для процессов** и управления памятью **для выделения/освобождения блоков** (allocator-level)? ⭐⭐⭐
- В чём принципиальная разница между внутренней и внешней фрагментацией, и как они проявляются при динамическом выделении? ⭐⭐⭐
- Как выбор стратегии учёта свободной памяти влияет на предсказуемость времени выделения (real-time vs general-purpose)? ⭐⭐⭐
- Как устроена битовая карта свободной памяти и что именно кодирует каждый бит? ⭐
- Что такое **allocation unit** и почему её размер определяет компромисс между расходом памяти на карту и потерями из-за фрагментации? ⭐⭐
- Почему слишком мелкая единица выделения делает карту «большой», но более точной? Какие практические последствия? ⭐⭐
- Почему слишком крупная единица выделения уменьшает размер карты, но увеличивает потери памяти? ⭐⭐
- Объясните на примере, почему поиск «N подряд свободных блоков» в битовой карте может быть медленным. ⭐⭐
- Какие аппаратные/низкоуровневые оптимизации могут ускорить поиск последовательности нулей в битовой карте? ⭐⭐⭐
- Почему битовые карты могут быть «простыми и надёжными», но «не всегда эффективными» для больших динамических систем? ⭐⭐
- В каких сценариях битовая карта может быть предпочтительнее связного списка (по нагрузке, паттерну выделений, требованиям к памяти)? ⭐⭐⭐

## Управление памятью на основе списков сегментов

- Как устроено представление памяти в виде списка сегментов (P/H), и какие поля должны храниться в узле списка? ⭐
- Почему список обычно сортируют по адресу, а не по размеру? Какие операции это упрощает? ⭐⭐
- Зачем при освобождении памяти нужно выполнять **слияние соседних дыр (merge)**, и что произойдёт, если его не делать? ⭐⭐
- Почему двусвязный список часто удобнее однонаправленного в менеджере памяти? ⭐⭐
- Какие сложности возникают при поддержании списка сегментов при частых выделениях и освобождениях? ⭐⭐
- Как связные списки помогают бороться с внешней фрагментацией (и где они бессильны)? ⭐⭐⭐
- Какие инварианты (правила корректности) должен поддерживать список сегментов, чтобы избежать «потери» памяти? ⭐⭐⭐
- Сравните битовые карты и связные списки по: памяти на метаданные, скорости поиска, удобству слияния свободных областей. ⭐⭐
- Почему битовая карта выигрывает по компактности при малом размере блока, но проигрывает по скорости поиска длинного свободного диапазона? ⭐⭐
- В каких ситуациях связный список будет медленнее битовой карты, и наоборот? ⭐⭐⭐
- Какие типы фрагментации сильнее выражены в подходе «список сегментов», а какие — в подходе «фиксированные блоки»? ⭐⭐⭐

## Алгоритмы выбора «дыры» (First/Next/Best/Worst Fit)

- Опишите идею First Fit и объясните, почему он часто считается практичным компромиссом. ⭐⭐
- В чём смысл Next Fit и почему он может вести себя хуже статистически, несмотря на «локальную оптимизацию»? ⭐⭐
- Почему Best Fit интуитивно кажется хорошим, но на практике часто приводит к большому числу маленьких «бесполезных дыр»? ⭐⭐⭐
- Почему Worst Fit обычно фрагментирует память быстрее ожидаемого? ⭐⭐⭐
- Какие метрики вы бы использовали для сравнения алгоритмов выделения? ⭐⭐
- Как поведение алгоритмов меняется, если запросы памяти имеют «типовые размеры» (например, часто 4К/8К/16К)? ⭐⭐
- Придумайте пример последовательности выделений/освобождений, где Best Fit даст хуже результат, чем First Fit. ⭐⭐⭐
- Придумайте пример, где First Fit создаёт неблагоприятную фрагментацию, а другой алгоритм справляется лучше. ⭐⭐⭐
- Почему алгоритмы «локально оптимальные» (Best Fit) могут быть «глобально плохими» в долгой работе системы? ⭐⭐⭐
- Зачем разделять структуры учёта на «список процессов» и «список дыр», и какие операции это ускоряет? ⭐⭐
- Почему сортировка списка дыр по размеру ускоряет Best Fit и сближает его по скорости с First Fit? ⭐⭐
- Какие новые проблемы появляются, если список дыр сортируется по размеру (а не по адресу)? ⭐⭐⭐
- Почему хранение метаданных прямо внутри «дыры» (размер + ссылка) удобно, и какие риски/ограничения у этого подхода? ⭐⭐
- Объясните идею Quick Fit и почему он особенно хорош для часто повторяющихся размеров запросов. ⭐⭐
- Почему в Quick Fit усложняется слияние свободных областей и растёт фрагментация? ⭐⭐⭐
- В каких типах систем Quick Fit может быть оправдан (и где он опасен)? ⭐⭐⭐

## Виртуальная память

- Какие проблемы остаются нерешёнными при base/limit, даже если изоляция процессов уже есть? ⭐⭐
- Почему рост размеров программ и желание держать «десятки программ открытыми» приводит к необходимости виртуальной памяти? ⭐⭐
- Чем swapping принципиально отличается от виртуальной памяти с точки зрения гранулярности перемещения данных? ⭐⭐
- Почему важно, что виртуальная память позволяет не блокировать систему целиком на долгих I/O-операциях (в сравнении с «грубыми» схемами)? ⭐⭐⭐
- Что такое overlays и почему в 1960-х их делали вручную? ⭐⭐
- Какие типичные ошибки возникали при ручном разбиении программы на overlays и управлении их подгрузкой? ⭐⭐⭐
- Почему overlays замедляли разработку и сопровождение программ, даже если экономили память? ⭐⭐
- Какие идеи overlays «перекочевали» в современную виртуальную память, но уже автоматизированно? ⭐⭐⭐
- Сформулируйте идею виртуальной памяти: что именно является «иллюзией» и за счёт чего она создаётся? ⭐⭐
- Почему деление адресного пространства на страницы фиксированного размера упрощает управление памятью? ⭐⭐
- Как MMU участвует в преобразовании виртуального адреса в физический, и почему без аппаратной поддержки это было бы слишком медленно? ⭐⭐⭐
- Что такое **page frame** и почему он должен быть того же размера, что и страница? ⭐⭐
- Объясните механизм **page fault**: что его вызывает и какие шаги выполняет ОС после возникновения. ⭐⭐⭐
- Почему виртуальная память позволяет запускать программы, размер которых больше физической RAM? ⭐⭐
- Почему виртуальная память обычно эффективнее swapping: в чём выигрыш по времени и по использованию памяти? ⭐⭐

## Устройство вирутальной памяти

- Почему виртуальный адрес логично делить на «номер страницы» и «смещение», и что делает каждую часть? ⭐⭐
- Как вычисляется физический адрес после того, как найден номер физического фрейма? ⭐⭐
- Какие ошибки проектирования могут возникать, если выбрать «слишком маленький» или «слишком большой» размер страницы? ⭐⭐⭐
- Что хранит таблица страниц и почему она является ключевой структурой виртуальной памяти? ⭐⭐
- Какие поля обычно есть в Page Table Entry (PTE) и зачем нужны: Present, Protection, Supervisor/User, Modified, Referenced? ⭐⭐⭐
- Почему биты Modified и Referenced особенно важны для политики замещения страниц? ⭐⭐⭐
- Объясните, как таблица страниц обеспечивает защиту: почему пользовательский процесс не может читать/писать страницы ядра. ⭐⭐⭐
- Почему многие ОС оставляют область около адреса 0 неотображённой (например, старт с 0x1000), и какие ошибки это помогает ловить? ⭐⭐
- Почему «одна плоская таблица страниц» становится неприемлемой в 64-битных адресных пространствах? ⭐⭐
- Объясните принцип многоуровневой таблицы страниц: почему «неиспользуемые подтаблицы не создаются» и как это экономит память. ⭐⭐⭐
- Почему многоуровневость увеличивает число обращений к памяти при трансляции адреса, и как TLB уменьшает эту цену? ⭐⭐⭐
- Какие последствия для производительности даёт промах TLB (TLB miss) и почему переключение контекста связано с TLB? ⭐⭐⭐

Ниже — **70 открытых экзаменационных вопросов** по лекции **«Управление памятью»** (фокус: переход к виртуальной памяти, включение paging на x86, многоядерность, TLB, shootdown, fault-и, IPT и Radix). После каждого вопроса — сложность ⭐ / ⭐⭐ / ⭐⭐⭐.

## Переход от физических адресов к виртуальным

- Почему ОС не может «просто включить виртуальную память» без предварительной подготовки структур данных? ⭐⭐
- Какие практические проблемы возникают, если процессор до включения paging продолжает исполнять код, но адреса уже интерпретируются иначе? ⭐⭐⭐
- Что меняется в модели исполнения программы, когда «все обращения к памяти становятся виртуальными»? ⭐⭐
- Почему переход к виртуальной памяти считается «точкой невозврата» в ранней загрузке ОС? ⭐⭐
- Какие минимальные условия должны быть выполнены, чтобы переход в виртуальный режим не привёл к мгновенному краху системы? ⭐⭐⭐
- Опишите пошагово, как ОС готовит таблицу страниц **до** включения paging и почему она должна содержать **физические** адреса. ⭐⭐⭐
- Почему в CR3 содержится физический, а не виртуальный адрес? ⭐⭐
- Что означает установка бита PG в CR0 и как это меняет путь адреса от инструкции до памяти? ⭐⭐
- Какие ошибки проектирования таблицы страниц чаще всего приводят к падению сразу после включения PG? ⭐⭐⭐
- Зачем в ранней фазе загрузки нужен минимальный набор отображений страниц, даже если позже карта памяти будет совсем другой? ⭐⭐
- Как бы вы объяснили смысл identity mapping человеку, который знает только «виртуальные адреса — это иллюзия»? ⭐⭐
- Почему identity mapping часто рассматривают как «мост» между физическим и виртуальным миром, и какие свойства у этого моста? ⭐⭐⭐

## Что происходит аппаратно при включении paging: pipeline flush и очистка TLB

- Почему при включении paging процессор выполняет **pipeline flush**, и что именно было бы «опасным» без этого шага? ⭐⭐⭐
- Каким образом уже декодированные инструкции в конвейере могут стать некорректными после смены модели адресации? ⭐⭐⭐
- Почему при включении paging очищается TLB, и какие последствия были бы, если бы старые трансляции сохранились? ⭐⭐
- Объясните, почему «следующая инструкция выполняется уже по виртуальному адресу» и какие гарантии нужны, чтобы она действительно была доступна. ⭐⭐⭐
- Какие дополнительные аппаратные события (кроме TLB flush и pipeline flush) вы ожидаете при переходе в новый режим памяти? ⭐⭐⭐
- Какие страницы ОС обязательно должна отобразить identity-mapping’ом, чтобы «не потерять себя» после включения paging? ⭐⭐⭐
- Почему page fault сразу после включения paging особенно опасен, и как identity mapping помогает его избежать? ⭐⭐
- Какие недостатки и риски несёт слишком долгоживущий identity mapping (если его не убрать вовремя)? ⭐⭐⭐
- Почему после успешного старта ОС обычно меняет схему отображения (kernel space / user space) и избавляется от временной карты? ⭐⭐

## Виртуальная память в многоядерных системах

- Почему у каждого ядра свой набор управляющих регистров (CR0/CR3/IDTR и др.), и какие преимущества это даёт ОС? ⭐⭐
- Как возможно, что одно ядро уже работает с PG=1, а другое — ещё с PG=0, и почему это важно при загрузке? ⭐⭐⭐
- Почему paging включается **каждым ядром отдельно**, даже если ОС «одна»? ⭐⭐
- Какие ошибки синхронизации могут возникнуть, если разные ядра имеют «разный взгляд» на память в раннем бутстрапе? ⭐⭐⭐
- Объясните роль bootstrap-ядра (BSP): что именно оно обязано подготовить для остальных ядер, чтобы они стали «равноправными». ⭐⭐⭐- Почему после включения питания активным становится только BSP, а остальные ядра «спят»? ⭐⭐
- Что такое INIT IPI и SIPI, и почему требуется два разных сигнала для пробуждения AP? ⭐⭐
- Почему адрес старта AP (в SIPI) должен быть < 1 МБ и как это связано с реальным режимом? ⭐⭐⭐
- Какие задачи должен выполнить BSP до пробуждения AP, чтобы те могли корректно стартовать? ⭐⭐
- Что произойдёт, если AP начнёт исполнять код без корректно настроенного стека? ⭐⭐⭐
- Опишите, в каком режиме находится AP сразу после SIPI и как формируется адресация в этот момент (Segment*16+Offset). ⭐⭐
- Почему у AP после пробуждения paging отключён и что это означает для доступа к памяти и коду загрузчика? ⭐⭐
- Какие шаги должен выполнить bootstrap-код AP, чтобы привести ядро к общему «режиму ОС» (protected mode + paging)? ⭐⭐⭐
- Почему bootstrap-код AP обычно размещают в низкой памяти (например, 0x7000–0x8000)? ⭐⭐
- Как вы организовали бы синхронизацию AP с BSP: какие флаги/барьеры/соглашения нужны, чтобы избежать гонок? ⭐⭐⭐
- Почему «после инициализации все ядра равноправны», и что остаётся общим ресурсом, несмотря на независимость регистров? ⭐⭐

## Переключение адресных пространств, кеши и безопасность

- Почему при смене CR3 сбрасывается TLB, но не обязательно сбрасываются L1/L2/L3 кеши? ⭐⭐⭐
- Чем полезно то, что кеши работают по физическим адресам, и какой «неочевидный минус» это создаёт для безопасности? ⭐⭐⭐
- Объясните принцип, как измерение времени доступа к данным в кеше может приводить к утечкам (на уровне идеи, без деталей конкретных атак). ⭐⭐⭐
- Почему даже «простая» операция смены CR3 может влиять на безопасность всей системы? ⭐⭐
- Зачем нужны техники вроде PCID/ASID, барьеры спекуляции и изоляция таблиц страниц ядра, если смотреть на проблему со стороны памяти и кешей? ⭐⭐⭐
- Почему многоуровневые таблицы страниц делают трансляцию адресов слишком медленной без дополнительного ускорения? ⭐⭐
- Что именно кэширует TLB и почему это «кэш другого типа», чем L1/L2 для данных? ⭐⭐
- Объясните разницу между TLB hit и TLB miss и их влияние на латентность каждого обращения к памяти. ⭐⭐
- Как устройство TLB как ассоциативного кеша влияет на скорость поиска и на стоимость промахов? ⭐⭐⭐
- Зачем разделять TLB для инструкций и данных (ITLB/DTLB), и какие преимущества/сложности это даёт? ⭐⭐
- Какие факторы в программе увеличивают вероятность TLB miss (паттерны доступа, размер working set, размер страниц)? ⭐⭐⭐
- Чем отличается hardware-managed TLB от software-managed, и почему это влияет на дизайн ядра ОС? ⭐⭐⭐
- Какие преимущества даёт hardware-managed подход (например, x86) с точки зрения простоты ОС, и какие минусы возможны? ⭐⭐
- Какие преимущества даёт software-managed подход (например, MIPS/SPARC) для контроля и оптимизаций ОС? ⭐⭐⭐
- Почему после заполнения TLB команда «повторяется», и какие свойства архитектуры делают это корректным? ⭐⭐⭐

## TLB и переключение контекста: flush, ASID/PCID, многоуровневые TLB

- Почему при переключении процесса «чужие записи» в TLB становятся опасными или бессмысленными? ⭐⭐
- Какие издержки у полной очистки TLB при каждом контекстном переключении и как это отражается на производительности? ⭐⭐
- Объясните идею ASID/PCID: как маркировка записей позволяет не очищать TLB полностью. ⭐⭐⭐
- Почему многоуровневые TLB (L1/L2) похожи на многоуровневые кеши, но решают отдельную задачу? ⭐⭐
- Какие сценарии приводят к тому, что даже большой L2 TLB не спасает производительность? ⭐⭐⭐
- Почему изменения в PTE «не отражаются мгновенно», и что именно продолжает использовать процессор? ⭐⭐
- Объясните, почему TLB хранит не только физический адрес, но и права доступа, и как это влияет на безопасность при смене разрешений. ⭐⭐⭐
- Почему правило «делать PTE менее разрешающим без flush нельзя» критично? Приведите логическое обоснование. ⭐⭐⭐
- Зачем нужен барьер памяти (mfence) между записью нового PTE и инвалидацией TLB? ⭐⭐⭐
- В чём отличие точечной инвалидации (invlpg) от глобальной (mov cr3, cr3) и как выбрать правильную стратегию? ⭐⭐
- Почему в многоядерной системе инвалидация TLB должна выполняться на **всех** ядрах, где могла кэшироваться страница? ⭐⭐⭐
- Объясните механизм TLB shootdown: зачем нужны IPI и подтверждения от ядер. ⭐⭐⭐
- Почему нельзя переиспользовать физический фрейм до завершения shootdown, даже если «мы уже обновили PTE»? ⭐⭐⭐
- Почему обработчик page fault сам подвержен риску page fault, и какие ресурсы ему критически нужны (стек, таблицы, буферы)? ⭐⭐⭐
- Что такое double fault (в смысле «исключение во время обработки исключения») и почему он часто заканчивается фатально для системы? ⭐⭐⭐
- Какие практики ОС используются, чтобы избежать page fault внутри обработчика (pinning/wired pages, prefaulting, отдельные пулы)? ⭐⭐⭐
- Почему разделение пулов страниц ядра и пользователя снижает вероятность катастрофических сбоев при вытеснении? ⭐⭐⭐

## Инвертированные таблицы страниц (IPT) и Radix Page Tables: эволюция масштабируемости

- Почему классические таблицы страниц начинают занимать «слишком много памяти» при больших виртуальных пространствах? ⭐⭐
- В чём идея inverted page table: что считается «ключом поиска» и почему запись соответствует физическому фрейму? ⭐⭐⭐
- Какие трудности создаёт поиск по хешу (коллизии, синхронизация) и почему это плохо для аппаратной реализации? ⭐⭐⭐
- Почему IPT со временем стали менее актуальны: какие технологические/архитектурные изменения «сдвинули баланс»? ⭐⭐
- Что такое Radix Page Tables как структура (radix tree) и чем она концептуально похожа на обычные многоуровневые таблицы? ⭐⭐
- Почему динамическая настраиваемая глубина дерева в Radix может быть преимуществом для больших систем? ⭐⭐⭐
- В чём идея выигрыша Radix в виртуализации по сравнению с подходами, где требуется «два обхода» таблиц? ⭐⭐⭐
- Почему при TLB hit производительность почти одинакова, а разница проявляется при TLB miss и в виртуализации? ⭐⭐
- Как поддержка разных размеров страниц влияет на эффективность TLB и на глубину обхода таблиц? ⭐⭐⭐
- Почему эволюция от hashed/IPT к radix/иерархиям считается «общим направлением» развития MMU? ⭐⭐⭐

Ниже — **65 открытых экзаменационных вопросов** по лекции **«Алгоритмы замещения страниц»** (≥50), **разделённых по темам**. После каждого вопроса — сложность ⭐ / ⭐⭐ / ⭐⭐⭐.

## Основы замещения страниц и цели политики

- Что происходит в ОС при page fault и почему почти всегда требуется освободить фрейм под новую страницу? ⭐⭐
- Чем принципиально отличается выселение **dirty**-страницы от **clean**-страницы с точки зрения стоимости? ⭐⭐
- Какие метрики вы бы использовали, чтобы оценивать «качество» алгоритма замещения страниц (не только число page faults)? ⭐⭐⭐
- Почему «минимизировать page faults» не всегда означает «минимизировать время выполнения»? ⭐⭐⭐
- Объясните феномен thrashing: как он связан с неудачным выбором жертвы и с рабочим множеством процесса? ⭐⭐⭐
- Какие системные сигналы (кроме page fault) могут подсказать ОС, что память перегружена и политика вытеснения работает плохо? ⭐⭐⭐
- Почему случайное замещение страниц обычно даёт плохие результаты, даже если кажется «справедливым»? ⭐⭐
- Приведите пример паттерна обращений, при котором случайный выбор почти гарантированно вызывает повторные page faults. ⭐⭐⭐
- Какие свойства поведения программ (локальность, фазы) делают задачу замещения принципиально предсказуемой? ⭐⭐
- Почему при одинаковом объёме памяти два разных приложения могут радикально отличаться по частоте page faults? ⭐⭐

## Алгоритмы Optimal Page Replacement и NRU

- В чём идея OPT и почему он «идеален» с точки зрения числа page faults? ⭐⭐
- Почему OPT нереализуем в реальной ОС, но всё равно полезен на практике? ⭐⭐
- Как можно смоделировать OPT на трассе обращений и что это даёт исследователю/инженеру? ⭐⭐
- Почему сравнение с OPT важно именно как «верхняя граница», а не как рецепт реализации? ⭐⭐
- Что означает утверждение «если алгоритм хуже OPT на 1–2%, то он почти оптимален» — и когда это может быть неверно? ⭐⭐⭐
- На последовательности обращений `[1,2,3,4,1,2,5,1,2,3,4,5]` объясните логику выбора жертвы OPT при памяти на 3 страницы (не обязательно считать все шаги, важен принцип). ⭐⭐⭐
- Какие данные о странице отражают биты **R (Referenced)** и **M (Modified)**, и почему их достаточно для базовой эвристики? ⭐⭐
- Зачем ОС периодически сбрасывает R-бит и почему выбор периода (например, 20 мс) влияет на качество алгоритма? ⭐⭐⭐
- Объясните 4 класса NRU (R/M) и почему именно такой приоритет обычно рационален. ⭐⭐
- Почему NRU выбирает случайную страницу внутри класса, и чем это полезно с точки зрения накладных расходов? ⭐⭐
- В каких сценариях NRU может вести себя плохо из-за того, что не различает «давно» и «совсем недавно»? ⭐⭐⭐
- Как изменится поведение NRU, если система слишком редко сбрасывает R-биты? А если слишком часто? ⭐⭐⭐
- Как можно реализовать/эмулировать R и M, если аппаратных битов нет? Почему это может быть дорого? ⭐⭐⭐
- Почему M-бит обычно не сбрасывают периодически, в отличие от R? ⭐⭐

## Алгоритмы FIFO и Second Chance

- Как FIFO выбирает жертву и почему он почти не требует вычислений? ⭐⭐
- В чём ключевой недостаток FIFO с точки зрения «актуальности» страниц? ⭐⭐
- Объясните аномалию Белади: почему увеличение числа фреймов может увеличить число page faults именно у FIFO? ⭐⭐⭐
- Приведите интуитивный пример (без строгого доказательства), как FIFO может вытеснить «полезную» страницу просто потому, что она старая. ⭐⭐
- Почему FIFO полезен как учебный алгоритм и как базовая линия для сравнения, но редко используется «в чистом виде»? ⭐⭐
- Как Second Chance модифицирует FIFO и какую проблему FIFO он решает? ⭐⭐
- Что происходит со страницей, у которой R=1, когда она рассматривается как кандидат на вытеснение? ⭐⭐
- Почему Second Chance можно считать «мягкой» версией NRU? ⭐⭐
- В каких условиях Second Chance вырождается по поведению почти в FIFO? ⭐⭐
- Какие накладные расходы появляются у Second Chance по сравнению с FIFO и от чего они зависят? ⭐⭐⭐

## Алгоритмы Clock и Enhanced Clock

- Почему Clock называют оптимизацией Second Chance и что именно оптимизируется (структуры данных/операции)? ⭐⭐
- Опишите роль «стрелки» в Clock: что она обозначает и как движется при page fault? ⭐⭐
- Почему Clock обычно проще масштабируется, чем вариант с реальной очередью FIFO и перемещением элементов? ⭐⭐
- Разберите пример: если у страниц A,B,C,D биты R = 1,1,0,1 и стрелка указывает на A — как Clock найдёт жертву? ⭐⭐⭐
- В каких нагрузках Clock особенно хорош, а в каких может оказаться недостаточно точным? ⭐⭐⭐
- Почему Clock так часто используется в реальных ОС (с точки зрения «цена/качество»)? ⭐⭐
- Как Enhanced Clock использует M-бит и почему это уменьшает число дорогих операций записи на диск? ⭐⭐⭐
- Чем Enhanced Clock концептуально похож на NRU и чем отличается на уровне «процесса поиска жертвы»? ⭐⭐
- Что даёт Two-Handed Clock по сравнению с обычным Clock: какую проблему точности он решает? ⭐⭐⭐
- Почему «две стрелки» позволяют лучше оценить, что страница действительно не использовалась достаточно долго? ⭐⭐⭐
- В каких сценариях Two-Handed Clock может дать заметный выигрыш, а где усложнение не оправдается? ⭐⭐⭐

## Алгоритмы LRU и NFU

- На каком предположении о поведении программ основан LRU и как это связано с локальностью? ⭐⭐
- Почему LRU часто рассматривают как практическое приближение OPT (интуиция «прошлое предсказывает будущее»)? ⭐⭐
- Как выглядела бы «идеальная» реализация LRU с глобальным счётчиком/временем и что в ней дорого? ⭐⭐⭐
- Почему чистый LRU редко реализуется напрямую в ОС без специальных аппаратных механизмов? ⭐⭐⭐
- На последовательности обращений `1,2,3,1,4,2,1,5` объясните принцип выбора жертвы LRU при 3 фреймах (важна логика, не обязательно полная таблица). ⭐⭐⭐
- Как работает NFU и какую «идею» (частота/популярность) он пытается использовать? ⭐⭐
- Почему NFU может сохранять страницы, которые были важны в прошлом, но больше не нужны? ⭐⭐⭐
- Как фазовое поведение программы (например, компилятор) может «сломать» NFU? ⭐⭐⭐
- Какие способы «забывания прошлого» вы могли бы предложить для улучшения NFU? ⭐⭐⭐

## Алгоритмы Aging и Working Set

- Опишите механизм Aging: сдвиг счётчика, добавление R в старший бит, сброс R. ⭐⭐
- Почему Aging лучше NFU имитирует LRU и как именно он «забывает» старые обращения? ⭐⭐
- От чего зависит точность Aging (размер счётчика, период тика) и какие компромиссы при выборе параметров? ⭐⭐⭐
- Почему Aging не различает порядок обращений внутри одного интервала, и когда это важно? ⭐⭐⭐
- Объясните, почему на практике часто достаточно 8-битного счётчика (с точки зрения «длины истории»). ⭐⭐
- Дайте определение рабочего множества W(k,t) и объясните смысл параметра окна (k или Δ). ⭐⭐
- Почему рабочее множество «меняется медленно» и как это связано с фазами выполнения программы? ⭐⭐
- Как алгоритм Working Set решает, что страница «вышла» из рабочего множества? Какие данные ему нужны? ⭐⭐⭐
- Почему Working Set помогает предотвращать thrashing на уровне всей системы, а не только одного процесса? ⭐⭐⭐
- Что такое prepaging и почему знание рабочего множества делает его возможным? ⭐⭐

## Алгоритмы WSClock и гибридные алгоритмы

- В чём ключевая идея WSClock и почему он считается удачным компромиссом точности и скорости? ⭐⭐⭐
- Какие поля хранит запись о странице в WSClock (R, M, время последнего использования) и зачем каждое нужно? ⭐⭐
- Что происходит, если WSClock находит «старую» страницу, но она dirty? Почему важна асинхронная запись? ⭐⭐⭐
- Опишите два сценария после полного обхода круга WSClock: когда были назначены записи и когда нет. ⭐⭐⭐
- Почему WSClock обычно уменьшает I/O по сравнению с простыми схемами, даже при высокой нагрузке? ⭐⭐⭐
- Почему современные ОС почти никогда не используют «чистые» алгоритмы, а строят гибриды (Clock/LRU/Working Set)? ⭐⭐⭐
- Зачем Linux разделяет страницы на файловые и анонимные (split LRU), и почему их нужно вытеснять по-разному? ⭐⭐⭐
- Как бы вы объяснили, почему политика замещения страниц тесно связана с подсистемой ввода-вывода и кэшированием файлов? ⭐⭐⭐
- Какие компромиссы возникают между справедливостью между процессами и глобальной оптимизацией page faults? ⭐⭐⭐
- Как бы вы спроектировали эксперимент, чтобы честно сравнить FIFO, Clock, Aging и WSClock на одной рабочей нагрузке? ⭐⭐⭐

Ниже — **60 открытых экзаменационных вопросов**, сгруппированных по темам. После каждого вопроса стоит **⭐–⭐⭐⭐** (сложность).

## Управление памятью на практике: базовые понятия и метрики, локальная и глобальная политика распределения страниц

- Что такое виртуальная память и какие проблемы она решает в ОС? ⭐
- Чем различаются *страница* (page) и *кадр/фрейм* (frame), и почему это различие принципиально? ⭐
- Какие цели преследует алгоритм замещения страниц, кроме “освободить место”? ⭐⭐
- Какие метрики производительности чаще всего используют для оценки работы подсистемы памяти (page fault rate, latency, throughput и т.п.) и почему? ⭐⭐
- Объясните, почему “меньше page fault” не всегда означает “быстрее система”. ⭐⭐
- В каких ситуациях стоимость page fault резко возрастает, и как это связано с I/O и состоянием dirty-страниц? ⭐⭐
- Какие компромиссы появляются при стремлении сделать подкачку “прозрачной” для процесса? ⭐⭐
- Почему управление памятью нельзя полностью “свести к выбору FIFO/LRU”? ⭐
- Какие ресурсы ОС расходуются на виртуальную память помимо самой RAM (структуры данных, swap, CPU-время)? ⭐⭐
- Как бы вы объяснили человеку без ОС-бэкграунда, почему память “заканчивается”, даже если на диске много места? ⭐⭐
- Сформулируйте ключевую разницу между **локальной** и **глобальной** политикой замещения страниц. ⭐
- Почему локальная политика обеспечивает изоляцию процессов, и какой ценой? ⭐⭐
- Приведите сценарий, когда при локальной политике процесс будет thrashing-иться, хотя в системе есть свободная память. ⭐⭐
- Как глобальная политика может улучшить утилизацию памяти при изменяющихся нагрузках? ⭐⭐
- Как глобальная политика может ухудшить предсказуемость производительности отдельного процесса? ⭐⭐
- Объясните, что означает “процессы конкурируют за кадры” в глобальной политике — на уровне последствий для latency и fairness. ⭐⭐
- Какие свойства системы (интерактивность, real-time, серверные нагрузки) обычно “предпочитают” локальную политику, а какие — глобальную? Обоснуйте. ⭐⭐⭐
- Почему алгоритмы, основанные на рабочем множестве (Working Set, WSClock), логически “локальные”? ⭐⭐
- FIFO и LRU могут работать в обеих политиках. Как изменится смысл “вытеснить самую старую/давно неиспользуемую” страницу при переходе от локальной к глобальной? ⭐⭐
- Какая политика лучше защищает важный системный процесс от деградации из-за шумного соседа, и как это связано с изоляцией? ⭐⭐

## Проблема распределения страниц и управление частотой ошибок (PFF), Thrashing

- Почему вопрос “сколько памяти выделить каждому процессу” принципиально сложен в многозадачной системе? ⭐⭐
- Объясните идею **Page Fault Frequency (PFF)**: что измеряем, какие решения принимаем, зачем. ⭐⭐
- Какие “плохие” эффекты возможны, если агрессивно добавлять страницы процессу при высоком PFF? ⭐⭐⭐
- А какие эффекты возможны, если агрессивно отбирать страницы при низком PFF? ⭐⭐⭐
- Как выбрать пороги PFF (верхний/нижний) и почему это зависит от типа нагрузки? ⭐⭐⭐
- Как PFF связан с понятием “рабочего множества”, даже если формально WS не вычисляется? ⭐⭐
- Почему ситуация “всем процессам нужно больше памяти, но забрать не у кого” является тревожным сигналом? ⭐⭐
- Какие источники “ложных” page faults могут искажать PFF (например, холодный старт, смена фаз программы)? ⭐⭐
- Чем отличается оптимизация распределения кадров “на уровне процессов” от оптимизации “на уровне страниц” (алгоритм замещения)? ⭐⭐
- Дайте точное определение **thrashing** и объясните, почему он возможен даже при “идеальном” алгоритме замещения. ⭐⭐
- Почему причина thrashing-а — это часто **суммарное рабочее множество**, а не “плохой LRU”? ⭐⭐
- Какие наблюдаемые симптомы в системе укажут на thrashing (CPU, диск, время отклика, очереди)? ⭐⭐
- Объясните, почему при thrashing CPU может быть “почти свободным”, а система при этом “умирает”. ⭐⭐
- Почему увеличение степени мультипрограммирования (больше процессов) иногда снижает производительность? ⭐⭐
- Как роль планировщика CPU пересекается с thrashing-ом (например, частые переключения контекста)? ⭐⭐
- Почему thrashing — это не только проблема конкретного процесса, но и системная? ⭐⭐
- Какие стратегии предотвращения thrashing вы считаете наиболее практичными в современных ОС и почему? ⭐⭐⭐

## Реакции ОС на нехватку памяти

- Опишите цель и принцип работы **OOM killer**: почему это “крайняя мера”. ⭐⭐
- Что такое “badness score” (идеологически): какие факторы логично учитывать, чтобы выбирать жертву? ⭐⭐
- Какие риски несёт OOM killer для целостности данных и пользовательского опыта? ⭐⭐
- В каких случаях завершение процесса лучше, чем swapping, и наоборот? ⭐⭐⭐
- Объясните swapping как “двухуровневое планирование”: что это означает в терминах активных/неактивных процессов. ⭐⭐
- Почему swapping может *вылечить* thrashing, хотя формально добавляет I/O? ⭐⭐
- Как ОС может определить, какой процесс выгоднее swap out, чтобы минимально ударить по отзывчивости? ⭐⭐⭐
- Почему “уменьшить нагрузку на память” часто означает “уменьшить число активных процессов”, а не “улучшить алгоритм замещения”? ⭐⭐
- В чём разница между **компактизацией**, **сжатием** и **deduplication** на уровне целей и затрат? ⭐⭐
- Как работает **same-page merging / deduplication**: какие страницы ищем, как храним, почему это экономит память? ⭐⭐
- Почему deduplication требует механизма **copy-on-write** при записи, и что будет без него? ⭐⭐
- Какие риски/минусы есть у deduplication (CPU overhead, задержки, безопасность/side-channel, ложные совпадения)? ⭐⭐⭐
- Объясните Copy-on-Write на примере `fork()` в UNIX: что помечается Read-Only и что запускает trap. ⭐⭐
- Почему COW ускоряет запуск процессов и экономит память именно “в среднем”, а не всегда? ⭐⭐

## Очистка страниц, выбор размера страницы

- Почему системе нужен **резерв чистых кадров**, и что происходит, если “почти все страницы dirty”? ⭐⭐
- Какую роль выполняет **paging daemon** и почему это фоновая задача, а не часть обработки каждого page fault? ⭐⭐
- Опишите жизненный цикл dirty-страницы: когда она становится dirty и когда/почему должна попасть на диск. ⭐⭐
- В чём идея **двух стрелок** (two-handed clock): что делает передняя рука и что делает задняя? ⭐⭐
- Почему two-handed clock повышает шанс, что “в момент надобности” найдётся чистая страница, и как это влияет на latency page fault? ⭐⭐⭐
- Как выбор интервалов пробуждения paging daemon влияет на производительность и “шум” I/O? ⭐⭐⭐
- Приведите пример, когда слишком агрессивная очистка страниц ухудшит производительность. ⭐⭐⭐
- Почему размер страницы — это компромисс между фрагментацией и накладными расходами таблиц страниц? ⭐⭐
- Объясните, что такое **внутренняя фрагментация** и почему она растёт при больших страницах. ⭐⭐
- Почему маленькие страницы увеличивают нагрузку на **таблицы страниц** и **TLB**? ⭐⭐
- Как большие страницы могут ускорять работу с диском (I/O) и почему это не всегда выигрывает? ⭐⭐
- В формуле перерасхода `overhead = s·e/p + p/2` объясните смысл каждого слагаемого “по-русски”. ⭐⭐⭐
- Как выводится оптимум `p = √(2se)` (по смыслу, без полного матанализа) и что он означает? ⭐⭐⭐
- Почему в примере с типичными параметрами получается около 4 КБ, и почему это стало “стандартом”? ⭐⭐
- Что такое **Transparent Huge Pages (THP)** и какие выгоды/риски они несут (например, latency spikes, фрагментация)? ⭐⭐⭐
- Почему “разные размеры страниц для разных частей системы” могут быть разумной стратегией? ⭐⭐

## Раздельные адресные пространства инструкций и данных (I-space / D-space)

- Какие ограничения единого адресного пространства были критичны в 16-битных системах? ⭐⭐
- Объясните идею разделения I-space и D-space: как это “удваивает” доступное адресное пространство. ⭐⭐
- Как аппаратно и системно реализуются два пространства (две таблицы страниц, выбор по типу доступа)? ⭐⭐
- Почему раздельные пространства позволяют “независимую подкачку” инструкций и данных, и зачем это могло быть полезно? ⭐⭐
- Почему сегодня эта идея “не нужна” для адресов, но продолжает жить в **I-cache/D-cache** и **I-TLB/D-TLB**? ⭐⭐
- Какие типичные конфликты/потери производительности уменьшаются при разделении кэшей для кода и данных? ⭐⭐⭐
- Почему код программы обычно безопасно шарить между процессами, а данные — нет? ⭐⭐
- Если несколько процессов используют общие страницы кода, какие механизмы учёта нужны ОС, чтобы корректно освобождать память? ⭐⭐
- Почему “поиск по всем таблицам страниц” дорог, и какие структуры данных логично использовать для учёта разделяемых страниц? ⭐⭐⭐
- Что должно произойти при завершении процесса, который делил страницы с другими: какие проверки обязательны? ⭐⭐
- Объясните, как `fork()` может давать “общие данные” безопасно с помощью Read-Only + trap + COW. ⭐⭐
- Приведите пример, когда COW может неожиданно привести к большому потреблению памяти (worst-case). ⭐⭐⭐

## Разделяемые библиотеки и Memory-mapped files

- Почему статическое связывание приводит к дублированию кода в памяти, даже если ядро поддерживает подкачку? ⭐⭐
- Опишите динамическое связывание: что такое “заглушки”, когда происходит связывание (при запуске/при первом вызове). ⭐⭐
- Почему shared libraries дают выигрыш и по диску, и по RAM одновременно? ⭐⭐
- Как shared libraries связаны с механизмом отображения файлов в память (mmap) на концептуальном уровне? ⭐⭐
- Почему проблема адресации возникает, если библиотека может быть загружена по разным адресам в разных процессах? ⭐⭐
- Что такое **position-independent code (PIC)** и как относительная адресация решает проблему перемещения? ⭐⭐⭐
- Какие последствия для безопасности и обновлений даёт динамическая библиотека по сравнению со статической? ⭐⭐
- Что значит “файл отображается в виртуальное адресное пространство” и почему это меняет модель программирования? ⭐⭐
- Опишите, что происходит при первом обращении к отображённой странице файла (demand paging и page fault). ⭐⭐
- Как ОС использует файл как “резервное хранилище” для страниц и чем это отличается от swap? ⭐⭐⭐
- Почему memory-mapped I/O может быть быстрее, чем read/write, и когда может быть медленнее? ⭐⭐⭐
- Как обеспечивается запись изменений обратно в файл: что означает modified/dirty в этом контексте? ⭐⭐
- Как два процесса могут использовать mmap одного файла как IPC: какие проблемы синхронизации/согласованности данных возникают? ⭐⭐⭐
- Почему shared libraries естественно реализуются через mmap, и что это даёт ОС? ⭐⭐

## Практическая реализация виртуальной памяти

- Какие задачи по памяти решает ОС на этапе **создания процесса** (таблицы страниц, swap, инициализация)? ⭐⭐
- Почему код иногда подкачивается прямо из исполняемого файла, а не из swap, и что это даёт? ⭐⭐
- Что происходит при **переключении процесса**: роль MMU, загрузка указателя на таблицу страниц, очистка TLB и теги процессов. ⭐⭐⭐
- Что такое **prepaging** и когда он помогает, а когда может навредить? ⭐⭐⭐
- Какие шаги выполняются при **завершении процесса** и почему shared pages усложняют освобождение ресурсов? ⭐⭐
- Почему page fault можно назвать “контролируемым прерыванием”, и что делает CPU/ОС на границе user/kernel? ⭐⭐
- Какие проверки прав доступа выполняются при page fault и почему “недопустимый адрес” ведёт к завершению процесса? ⭐⭐
- Опишите полный пайплайн page fault (от ловушки до повторного выполнения инструкции) и укажите, где возможны блокировки на I/O. ⭐⭐⭐
- Почему запись dirty-страницы перед вытеснением может приостановить процесс и как это влияет на планирование? ⭐⭐⭐
- Как ОС гарантирует, что после page fault выполнение продолжится “как ни в чём не бывало”, сохраняя изоляцию процессов? ⭐⭐⭐
- Почему возобновление инструкции после fault — отдельная сложная проблема, а не “просто повторить instruction”? ⭐⭐⭐
- Как автоинкремент/автодекремент регистров усложняет откат или повтор инструкции, и чем опасна ошибка восстановления? ⭐⭐⭐
- Какие виды аппаратной поддержки могут помочь ОС корректно возобновлять сложные инструкции? ⭐⭐⭐
- Объясните “баланс между сложностью процессора и нагрузкой на ОС” на примере механизма восстановления инструкций. ⭐⭐⭐

## Виртуальная память, I/O, DMA

- Почему виртуальная память нельзя рассматривать “в изоляции” от ввода-вывода в реальной ОС? ⭐⭐
- Опишите типичный сценарий: процесс делает `read()` в пользовательский буфер и блокируется. Какие риски для страниц этого буфера появляются, пока CPU выполняет другие процессы? ⭐⭐
- Почему глобальная политика замещения повышает вероятность конфликтов между I/O и заменой страниц по сравнению с локальной? ⭐⭐
- Какие события в системе могут привести к тому, что страница пользовательского буфера станет кандидатом на вытеснение во время I/O? ⭐⭐
- Чем отличается I/O “в память процесса” от I/O “в буфер ядра” с точки зрения управления страницами и безопасности данных? ⭐⭐
- Какие требования к ОС возникают из-за того, что I/O может длиться долго, а алгоритм замещения работает постоянно? ⭐⭐⭐
- Почему задача “защитить буфер во время I/O” — это часть дизайна системы замещения страниц, а не отдельная оптимизация? ⭐⭐⭐
- Что такое DMA и почему он радикально меняет роль CPU в передаче данных? ⭐
- Почему DMA особенно важен при больших объёмах данных (диск, сеть, видео), а не только “для скорости вообще”? ⭐⭐
- Объясните, почему DMA создаёт принципиальную опасность при вытеснении страниц, даже если алгоритм замещения “правильный”. ⭐⭐⭐
- Что именно может произойти, если страница буфера выгружается во время DMA (опишите сценарий частичной записи и повреждения данных)? ⭐⭐⭐
- Почему такая ошибка может проявляться как “случайная порча памяти” и быть крайне трудной для отладки? ⭐⭐⭐
- Какой компонент (CPU/MMU/контроллер DMA/ядро ОС) “виноват” в том, что DMA не понимает виртуальные адреса? ⭐⭐
- Почему DMA работает с физическими адресами, и почему нельзя просто “дать ему виртуальные”? ⭐⭐
- Какие накладные расходы возникают, если отказаться от DMA и заставить CPU копировать данные вручную? ⭐⭐

## Защита страниц во время I/O, особенности работы DMA

- Что означает блокировка страницы (locking/pinning) в контексте замещения страниц? ⭐⭐
- Почему pinning считается “надёжным” решением для DMA и не требует копирования данных? ⭐⭐
- Какие условия должны выполняться, чтобы ОС могла безопасно снять блокировку страницы после завершения I/O? ⭐⭐
- Какие системные проблемы появляются, если слишком много страниц закреплено (pinned) одновременно? ⭐⭐⭐
- Почему pinning может “умеренно увеличить задержки подкачки”, даже если он защищает данные? ⭐⭐
- Какие страницы типично блокируются в ОС помимо пользовательских буферов (например, метаданные, буферы драйверов)? ⭐⭐⭐
- Почему pinning усложняет работу алгоритма замещения (например, уменьшается число кандидатов на вытеснение)? ⭐⭐⭐
- Какие ошибки проектирования могут привести к “утечке pinned-страниц” и деградации системы? ⭐⭐⭐
- Сравните pinning с “запретом выгрузки” на уровне процесса: чем отличается по эффекту и рискам? ⭐⭐
- Как работает схема “I/O в буфер ядра, потом копирование в память процесса”? ⭐⭐
- Почему это решение проще с точки зрения корректности, но хуже по производительности? ⭐⭐
- В каких случаях ОС вынуждена использовать kernel buffering вместо pinning (приведите возможные причины)? ⭐⭐⭐
- Почему лишнее копирование особенно дорого при высоких скоростях устройств (NVMe, 10/100GbE)? ⭐⭐
- Как kernel buffering влияет на кэширование страниц и вероятность thrashing? ⭐⭐⭐
- Почему непрерывный виртуальный буфер может соответствовать разнесённым физическим страницам? ⭐⭐
- Объясните, почему “разнесённость физических страниц” осложняет DMA-передачу. ⭐⭐
- Что такое scatter/gather DMA и какую проблему он решает? ⭐⭐
- Почему список сегментов (физический адрес + длина) позволяет устройству выполнить одну “логическую” передачу как серию физических? ⭐⭐
- В чём смысл batching: почему одно прерывание по завершении всей серии передач лучше, чем много прерываний? ⭐⭐
- Как scatter/gather снижает накладные расходы CPU и повышает пропускную способность I/O? ⭐⭐
- Почему scatter/gather полезен не только для пользовательских буферов, но и для подкачки страниц (page-in/page-out)? ⭐⭐⭐
- Какие риски для безопасности/изоляции возникают, если ОС неверно сформирует scatter/gather список? ⭐⭐⭐

## Backing Store: swap как основа виртуальной памяти

- Что такое backing store и почему без него виртуальная память теряет смысл (в общем случае)? ⭐⭐
- Чем swap-раздел отличается от swap-файла с точки зрения адресации и управления блоками? ⭐⭐
- Почему отдельный swap-раздел может быть быстрее и проще для ОС, чем файл подкачки? ⭐⭐
- В чём ограничение “классической схемы”, где процессу выделяют фиксированное место в swap заранее? ⭐⭐
- Почему в фиксированной схеме сложно поддержать рост стека/данных процесса? ⭐⭐⭐
- Объясните идею разделения swap по областям (код/данные/стек). Какие плюсы это даёт? ⭐⭐⭐
- Что означает динамическая модель: “страница получает место в swap только при выгрузке”? ⭐⭐
- Почему динамическая модель экономит место на диске, но усложняет управление? ⭐⭐
- Что такое disk map и какие данные он должен хранить, чтобы динамический swap работал корректно? ⭐⭐⭐
- Какие типы страниц обычно **не нужно** сохранять в swap (например, код программы/библиотеки) и почему? ⭐⭐
- Почему выгрузка кода и библиотек “без сохранения” может ускорить систему и уменьшить swap? ⭐⭐
- Как наличие memory-mapped файлов меняет роль swap и понятие “резервного хранилища”? ⭐⭐⭐
- Сформулируйте принцип разделения политики и механизма применительно к подкачке страниц. ⭐⭐
- Почему такой принцип повышает модульность и переносимость ОС? ⭐⭐
- Опишите архитектуру Mach: какие роли выполняют MMU handler, page fault handler и external pager? ⭐⭐⭐
- Как выглядит обработка page fault в Mach, если политика вынесена во внешний пейджер? ⭐⭐⭐
- Почему “идеально” было бы реализовать алгоритм замещения во внешнем пейджере? ⭐⭐
- Почему внешний пейджер не может напрямую реализовать LRU “как в учебнике”? (роль R/M битов) ⭐⭐⭐
- Какие компромиссы возможны: ядро передаёт статистику или оставляет алгоритм замещения внутри себя? ⭐⭐⭐
- Почему обмен сообщениями между ядром и external pager снижает производительность? ⭐⭐
- Какие современные аналоги идеи внешнего управления page fault существуют (например, пользовательские обработчики событий), и зачем они нужны? ⭐⭐⭐

## Сегментация памяти

- Почему одномерная (линейная) модель виртуальной памяти неудобна для программ с независимыми растущими областями (код/данные/стек)? ⭐⭐
- Что такое сегментация и как адрес вида `<номер сегмента, смещение>` меняет мышление об адресах? ⭐⭐
- Какие преимущества сегментации дают реальную пользу компоновке, модульности и защите? ⭐⭐⭐
- Почему сегментация естественно поддерживает разделение кода между процессами? ⭐⭐
- Какие трудности возникают с указателями при сегментации (арифметика, сравнение, передача между модулями)? ⭐⭐⭐
- Почему near/far pointers в старых x86 были “болью программиста”, и что это говорит о цене гибкости? ⭐⭐⭐
- Объясните происхождение термина “segmentation fault” и почему он остался, хотя механизмы изменились. ⭐⭐
- Приведите несколько типичных причин segfault в современных ОС и объясните их на уровне защиты памяти. ⭐⭐
- Почему сегменты переменного размера приводят к появлению “дыр” в памяти со временем? ⭐⭐
- Что такое внешняя фрагментация и почему общий объём свободной памяти может быть достаточным, но всё равно не хватать места? ⭐⭐
- Что такое checkerboarding и как оно связано с деградацией использования памяти? ⭐⭐⭐
- Как работает компактизация: какие действия должна выполнить ОС, чтобы собрать свободную память в один блок? ⭐⭐⭐
- Почему компактизация дорогая (приостановка процессов, обновление таблиц, пересчёт адресов)? ⭐⭐⭐
- Почему чистая сегментация редко используется “в лоб” в современных ОС? ⭐⭐

## Сегментация + пейджинг

- Почему крупные сегменты нельзя эффективно держать целиком в памяти, и как пейджинг внутри сегмента решает проблему? ⭐⭐
- Что такое сегментно-страничная организация памяти и какие преимущества она объединяет? ⭐⭐⭐
- Объясните, почему TLB позволяет рабочим множествам выполняться “почти без замедлений”, если они помещаются в кэш трансляций. ⭐⭐⭐
- Как в x86-32 преобразуется адрес “сегмент:смещение” в линейный адрес, а затем в физический? ⭐⭐⭐
- Какую роль играли GDT/LDT и что хранит дескриптор сегмента (base/limit/access bits)? ⭐⭐⭐
- Почему большинство ОС (UNIX/Linux/Windows) фактически игнорировали сегментацию и опирались на пейджинг? ⭐⭐
- Почему сегментация мешала переносимости ПО и дизайну ОС? ⭐⭐⭐
- Что означает “плоская модель памяти” в x86-64 и почему она упростила жизнь разработчикам? ⭐⭐
- Почему сегменты FS/GS остались активными, и что такое TLS в этом контексте? ⭐⭐
- Объясните, почему доступ через `fs:[offset]` или `gs:[offset]` ускоряет работу с thread-local данными. ⭐⭐⭐

## Зачем нужны файловые системы. Накопители и блочный интерфейс

- Почему хранение данных только в оперативной памяти не подходит для большинства реальных приложений? ⭐
- Какие свойства долговременного хранения делают его необходимым для баз данных и серверных систем? ⭐⭐
- Какие проблемы возникают, если несколько процессов должны работать с одной и той же информацией без файловой системы? ⭐⭐
- Объясните, почему «сохранение данных после сбоя питания» — это отдельное требование, а не просто «сохранение на диск». ⭐⭐⭐
- В каких ситуациях хранение данных только в RAM может быть оправдано? Приведите примеры. ⭐⭐
- Почему удобство для пользователя является важной задачей файловой системы, а не только приложений? ⭐⭐
- Как связаны требования многозадачности ОС и необходимость файловой системы? ⭐⭐
- Какие виды данных (по масштабу и длительности хранения) наиболее типичны для файловых систем? ⭐
- Почему HDD и SSD, несмотря на разные технологии, часто предоставляют одинаковый интерфейс «Read block / Write block»? ⭐⭐
- Что означает «блочное устройство» и почему файловые системы строятся поверх блоков? ⭐⭐
- Почему блочные операции неудобны для пользователя и приложений напрямую? ⭐
- Чем отличается последовательный характер работы ленты от диска, и как это повлияло на развитие файловых систем? ⭐⭐
- Как многозадачность усложняет работу с диском на уровне «сырых блоков»? ⭐⭐
- Какие свойства SSD влияют на проектирование файловых систем (например, скорость, износ, особенности записи)? ⭐⭐⭐
- Какие проблемы могут возникнуть, если приложения сами начнут «распределять блоки» без участия ОС? ⭐⭐⭐

## Абстракция файла, имена файлов

- Дайте определение файла как абстракции и объясните, что именно она скрывает от пользователя. ⭐
- Почему важно, что файл существует независимо от процесса, который его создал? ⭐⭐
- В чём отличие «файл исчезает только по инициативе владельца» от «файл исчезает при завершении программы»? ⭐
- Какие операции над файлами являются фундаментальными для любой ОС и почему? ⭐⭐
- Чем файл отличается от «просто области на диске» с точки зрения ОС? ⭐⭐
- Почему файловая система считается частью ОС, а не обычной библиотекой? ⭐⭐⭐
- Как понятие файла помогает обеспечивать совместное использование данных разными программами? ⭐⭐
- Зачем вообще нужны имена файлов, если на диске данные хранятся в блоках с номерами? ⭐
- Почему ограничения старых систем (например, 8.3 в MS-DOS) влияли на стиль разработки ПО и организации данных? ⭐⭐
- Объясните последствия различий в чувствительности к регистру между UNIX и Windows при переносе проектов. ⭐⭐
- Почему современные ОС ограничивают имя файла (например, 255 символов), даже если теоретически можно больше? ⭐⭐⭐
- Какие проблемы могут возникнуть при использовании спецсимволов и пробелов в именах файлов? ⭐⭐
- Чем отличается «уникальность имени файла» в одноуровневом каталоге и в иерархии каталогов? ⭐⭐
- Почему имя файла не должно быть единственным источником истины о содержимом файла? ⭐⭐

## Расширения и типы файлов, структура файлов

- Почему расширение файла в UNIX — это соглашение, а в Windows — часть пользовательского опыта и системы? ⭐⭐
- Приведите примеры ситуаций, когда расширение вводит пользователя в заблуждение или создаёт риск безопасности. ⭐⭐
- Почему ОС должна уметь распознавать хотя бы исполняемые файлы? ⭐⭐
- Как утилита `file` в *nix определяет тип файла по содержимому, и почему это надёжнее расширения? ⭐⭐⭐
- Объясните идею «магического числа» в заголовке бинарного файла и зачем оно нужно. ⭐⭐⭐
- Почему типизация файлов на уровне ОС может мешать пользователю и разработчику? ⭐⭐⭐
- Как бы вы объяснили различие между «файл — это данные» и «файл — это данные + метаданные» на примерах? ⭐⭐
- Почему современные ОС чаще всего рассматривают файл как поток байтов? ⭐⭐
- Какие преимущества и недостатки у модели «последовательность байтов» для приложений? ⭐⭐
- В каких системах и сценариях могла быть полезна модель «последовательность записей фиксированной длины»? ⭐⭐
- Почему структура «дерево записей» подходит для банковских/коммерческих систем? ⭐⭐⭐
- Чем отличается ответственность ОС и приложения в модели «поток байтов»? ⭐⭐
- Почему «универсальность» часто важнее «оптимальности для частного случая» при выборе структуры файла? ⭐⭐⭐
- Как принципы древовидных файлов применяются сегодня внутри баз данных, даже если ОС не предоставляет такой интерфейс? ⭐⭐⭐
- Какие типы файлов выделяют UNIX-подобные системы и зачем нужны файлы устройств? ⭐⭐
- Что означает идея «в UNIX всё — файл», и какие практические преимущества она даёт? ⭐⭐
- Чем каталог отличается от обычного файла с точки зрения назначения и структуры? ⭐⭐
- Почему бинарный файл выглядит как «хаос» при выводе на экран, но остаётся строго структурированным для программы? ⭐
- Какие риски возникают при попытке обрабатывать бинарные данные как текст? ⭐⭐
- Как ОС может запускать исполняемый файл, если «не знает, что внутри», а видит только байты? ⭐⭐⭐

## Доступ к содержимому файла. Атрибуты файла

- Почему исторически сначала появился последовательный доступ, и как носители данных повлияли на это? ⭐⭐
- В чём принципиальная разница между последовательным и произвольным доступом? ⭐
- Почему операция `seek` является ключевой для произвольного доступа? ⭐⭐
- Приведите примеры задач, где последовательный доступ эффективнее произвольного. ⭐⭐
- Почему произвольный доступ критичен для баз данных и редакторов? ⭐⭐
- Какие ошибки проектирования приложения могут возникнуть, если разработчик «случайно» полагается на последовательный доступ? ⭐⭐⭐
- Что может означать «после seek можно читать последовательно», и почему это удобно? ⭐⭐
- Какие атрибуты файла вы считаете минимально необходимыми для любой файловой системы и почему? ⭐⭐
- Как метаданные помогают обеспечивать безопасность и контроль доступа? ⭐⭐
- Почему время последнего доступа и время последнего изменения — это разные вещи, и где это важно? ⭐⭐⭐
- Что такое права доступа в UNIX-подобных системах и как они влияют на операции с файлом? ⭐⭐
- Почему «владелец» и «группа» являются важными атрибутами в многопользовательской ОС? ⭐⭐
- Объясните смысл флагов Read-only, Hidden, System, Archive, Temporary и их практическое применение. ⭐⭐
- Почему некоторые атрибуты ОС обязана обновлять автоматически, а не оставлять это приложению? ⭐⭐⭐
- Разберите вывод `ls -l`: какие поля там отражают состояние файла и как они используются системой? ⭐⭐

## Операции над файлами. Каталоги

- Какие этапы проходит ОС при выполнении `Open`, и зачем она «загружает атрибуты и адреса блоков в память»? ⭐⭐⭐
- Почему важно закрывать файлы (`Close`), даже если программа скоро завершится? ⭐⭐
- Чем `Write` отличается от `Append` с точки зрения риска потери данных? ⭐⭐
- В каких случаях `Append` может быть реализован через `Seek + Write`, и почему это не всегда эквивалентно? ⭐⭐⭐
- Почему `Rename` обычно не требует копирования данных и считается «дешёвой» операцией? ⭐⭐
- Что делает `Delete` на уровне файловой системы помимо удаления имени? ⭐⭐⭐
- Как `Get attributes` используется утилитами сборки (например, `make`) и почему это важно для автоматизации? ⭐⭐
- Какие ошибки может допустить программа при обработке результата `read()` и `write()`? ⭐⭐⭐
- Почему каталог называют «специальным файлом», и что именно в нём хранится? ⭐⭐
- Что означает «каталог сопоставляет имя файла и данные для доступа», и почему это ключевая функция? ⭐⭐
- Какие ограничения и проблемы возникают в одноуровневой системе каталогов при росте числа файлов? ⭐⭐
- Почему одноуровневые каталоги могут быть полезны в embedded-устройствах и простых системах? ⭐⭐
- Чем иерархическая структура каталогов решает проблему масштабирования? ⭐⭐
- Почему домашний каталог пользователя важен для изоляции данных и безопасности? ⭐⭐
- Как исторически Multics повлияла на современные файловые системы? ⭐⭐

## Пути к файлам. Операции над каталогами.

- Чем абсолютный путь отличается от относительного, и когда каждый предпочтительнее? ⭐
- Почему абсолютные пути считаются более надёжными для программ? ⭐⭐
- Что такое рабочий каталог процесса и почему он влияет на интерпретацию относительных путей? ⭐⭐
- Какие ошибки и уязвимости могут возникнуть, если программа использует относительные пути «вслепую»? ⭐⭐⭐
- Зачем нужны специальные имена `.` и `..`, и как они помогают навигации? ⭐⭐
- Почему библиотечные функции почти никогда не меняют рабочий каталог процесса? ⭐⭐⭐
- Почему `rmdir` обычно разрешает удалять только пустой каталог? ⭐⭐
- Зачем при создании каталога автоматически добавляются записи `.` и `..`? ⭐⭐
- Почему нельзя (или нежелательно) читать каталог обычным `read()`, и зачем нужны `opendir/readdir/closedir`? ⭐⭐⭐
- Что именно возвращает `readdir`, и почему это удобно для переносимости программ? ⭐⭐
- Почему переименование каталога обычно дешёвое и не требует копирования содержимого? ⭐⭐

## Файловые ссылки

- Что такое жёсткая ссылка и почему она увеличивает счётчик ссылок в inode? ⭐⭐
- Почему файл удаляется только тогда, когда счётчик ссылок становится равным 0? ⭐⭐
- Почему hard link нельзя делать на каталоги в UNIX, и какую проблему это предотвращает? ⭐⭐⭐
- Чем `unlink` отличается от «удалить файл как данные», и почему это важно понимать? ⭐⭐
- Объясните, почему hard links работают только в пределах одного диска/раздела. ⭐⭐⭐
- Что такое символическая ссылка и почему её называют «маленьким файлом с путём»? ⭐⭐
- Почему symbolic links гибче hard links, но обычно медленнее? ⭐⭐
- Что такое «сломанная» символическая ссылка и как она возникает? ⭐⭐
- Какие циклы могут возникать из-за symbolic links и чем это опасно для утилит обхода каталогов? ⭐⭐⭐
- Почему наличие нескольких имён у одного файла превращает структуру файловой системы из дерева в DAG? ⭐⭐⭐
- Какие сложности возникают у резервного копирования (backup), если один и тот же файл достижим по нескольким путям? ⭐⭐⭐
- Почему при переносе данных на другую систему структура ссылок может быть утеряна, и к чему это приводит? ⭐⭐⭐
- Как бы вы реализовали «умный backup», чтобы не копировать один и тот же файл дважды? ⭐⭐⭐

## Внутреннее устройство файловой системы на диске. Загрузка ОС

- Какие компоненты обычно присутствуют в разделе файловой системы (boot block, superblock, free space, inode area, data area) и зачем каждый нужен? ⭐⭐⭐
- Почему внутреннее устройство файловой системы отличается от пользовательского интерфейса, и почему это важно? ⭐⭐
- Что такое superblock и какие параметры файловой системы он должен хранить? ⭐⭐⭐
- Почему хранение информации о свободных блоках является критически важной задачей? ⭐⭐
- Опишите последовательность загрузки BIOS → MBR → boot block → OS и роль каждого этапа. ⭐⭐⭐
- Какие ограничения у MBR и почему они стали проблемой для современных дисков? ⭐⭐
- Что такое GPT и почему её копируют в начало и конец диска? ⭐⭐⭐
- Почему UEFI называют «мини-операционной системой», и какие функции это подтверждают? ⭐⭐⭐
- Что такое EFI System Partition (ESP), зачем она нужна и почему часто форматируется в FAT32? ⭐⭐⭐

## Размещение файлов. Реализация каталогов

- В чём идея непрерывного размещения и почему оно быстрое для чтения? ⭐⭐
- Что такое внешняя фрагментация и почему она разрушает преимущества непрерывного размещения? ⭐⭐⭐
- Почему требование «заранее знать размер файла» делает непрерывное размещение неудобным для современных приложений? ⭐⭐
- Объясните идею linked-list allocation и почему оно устраняет внешнюю фрагментацию. ⭐⭐
- Почему linked-list allocation делает произвольный доступ крайне медленным? ⭐⭐⭐
- В чём заключается идея FAT и как перенос указателей в таблицу ускоряет работу? ⭐⭐⭐
- Почему FAT плохо масштабируется на большие диски и требует много памяти? ⭐⭐⭐
- Что такое inode и почему он не хранит имя файла? ⭐⭐⭐
- Какие метаданные и ссылки на блоки обычно находятся в inode? ⭐⭐
- Почему хранение в памяти только inode открытых файлов — это преимущество по сравнению с FAT? ⭐⭐⭐
- Как косвенные блоки (single/double/triple indirect) расширяют максимальный размер файла? ⭐⭐⭐
- Какие варианты структуры каталога существуют (фиксированные записи, переменная длина, куча имён) и чем они отличаются? ⭐⭐⭐
- Почему фиксированная длина имени в записи каталога приводит к потере места? ⭐⭐
- Какие проблемы возникают у каталогов с переменной длиной записей при удалении файлов? ⭐⭐⭐
- В чём смысл подхода «фиксированные записи + куча имён», и какие сложности он добавляет? ⭐⭐⭐
- Почему хеш-таблицы ускоряют поиск в больших каталогах и какие минусы у этого решения? ⭐⭐⭐
- Что такое кеширование результатов поиска в каталогах и когда оно даёт максимальный эффект? ⭐⭐

## Log-Structured File System

- Почему рост объёма дисков не означает пропорционального роста скорости файловых операций? ⭐⭐
- Почему операция **seek** становится узким местом именно для классических HDD-файловых систем? ⭐⭐
- Объясните, почему в современных системах «большинство чтений обслуживается из кэша» и как это влияет на дизайн ФС. ⭐⭐
- Почему большое число **маленьких записей** считается худшей нагрузкой для традиционных файловых систем? ⭐⭐⭐
- В чём принципиальная идея LFS «диск как большой лог» и что она меняет в философии записи? ⭐⭐
- Почему LFS буферизует изменения в RAM перед записью на диск, и какие риски/ограничения это создаёт? ⭐⭐⭐
- Чем запись «крупным сегментом» принципиально отличается от множества маленьких обновлений метаданных? ⭐⭐
- Какие типы данных могут оказаться внутри одного сегмента LFS и зачем их «смешивают»? ⭐⭐⭐
- Что такое **segment summary**, какую информацию он должен содержать и почему он критически важен? ⭐⭐⭐
- Почему в LFS нельзя «вычислить адрес inode по номеру», как в классическом UNIX-подходе? ⭐⭐
- Что такое **i-node map**, и какую проблему она решает в LFS? ⭐⭐
- Почему i-node map хранится и на диске, и в RAM-кэше? ⭐⭐
- Какие последствия для производительности будут, если i-node map плохо кэшируется? ⭐⭐⭐
- Как меняется процесс поиска файла по имени (через каталог) в LFS по сравнению с традиционной inode-системой? ⭐⭐⭐
- Какие данные являются «истиной» о последней версии inode в LFS, и почему это важно при сбоях? ⭐⭐⭐

## Переполнение лога и производительность LFS

- Почему лог в LFS не может расти бесконечно и что происходит со «старыми версиями» данных? ⭐⭐
- Что означает, что старые сегменты содержат устаревшие версии блоков? Приведите пример. ⭐⭐
- Какую задачу выполняет **Cleaner Thread** и почему без него LFS неработоспособна? ⭐⭐
- Как cleaner определяет, какие данные в сегменте «живые», а какие «мёртвые»? ⭐⭐⭐
- Почему cleaner переписывает живые данные в новый сегмент, а не «удаляет старые на месте»? ⭐⭐⭐
- Объясните идею «лог превращается в кольцевой буфер»: что является «головой», а что «хвостом»? ⭐⭐
- Какие сценарии могут привести к тому, что cleaner станет узким местом производительности? ⭐⭐⭐
- Как вы думаете, как влияет выбор размера сегмента (например, 1 МБ) на эффективность LFS? ⭐⭐⭐
- Почему LFS может ускорять маленькие записи до 10 раз? За счёт каких эффектов? ⭐⭐⭐
- Почему чтение в LFS может быть «на уровне UNIX» несмотря на необычную структуру хранения? ⭐⭐⭐
- В каких типах рабочих нагрузок LFS особенно эффективна, а в каких может проигрывать? ⭐⭐⭐
- Какие компромиссы по сложности реализации появляются при переходе от классической ФС к LFS? ⭐⭐⭐
- Почему идеи LFS считаются фундаментом для современных log-structured подходов и journaling? ⭐⭐⭐

## Журналирование

- Почему даже «простая операция» в ФС на практике состоит из нескольких шагов? ⭐⭐
- Что такое «коррупция файловой системы» после сбоя, и почему она возникает между шагами операции? ⭐⭐
- Почему при сбоях особенно важно гарантировать целостность **метаданных**, а не только пользовательских данных? ⭐⭐⭐
- Какие типичные причины падений ОС приводят к необходимости журналирования? ⭐
- Чем журналирование отличается от резервного копирования (backup) по целям и механизму? ⭐⭐
- Опишите общий алгоритм журналирования: что происходит **до** выполнения операции и что **после**? ⭐⭐
- Почему операции «не выполняются сразу», а сначала записываются в журнал? ⭐⭐
- Что именно должно быть записано в журнал: «намерение», «изменения», «новые значения»? Обоснуйте. ⭐⭐⭐
- Почему запись из журнала удаляется после успешного завершения операции, а не хранится всегда? ⭐⭐
- Что происходит при загрузке системы, если журнал содержит незавершённую операцию? ⭐⭐
- Какие риски появляются, если журнал сам повреждён или частично записан? ⭐⭐⭐

## Журналирование. Детали.

- Какие три шага включает удаление файла в UNIX-подобной системе и почему они логически разделены? ⭐⭐
- Приведите несколько вариантов повреждений ФС, если сбой произойдёт между шагами удаления файла. ⭐⭐⭐
- Почему ситуация «файл исчез из каталога, но блоки не освобождены» опасна для системы? ⭐⭐
- Почему ситуация «блоки освобождены, но inode ещё указывает на них» опаснее, чем кажется? ⭐⭐⭐
- Что такое идемпотентная операция в контексте журналирования и почему это ключевое требование? ⭐⭐⭐
- Почему «освободить блок n» считается безопасной повторяемой операцией? ⭐⭐
- Почему «удалить запись из каталога» можно сделать идемпотентной операцией? ⭐⭐⭐
- Почему «добавить блок в конец списка свободных» не является идемпотентным действием? ⭐⭐⭐
- Какие структуры данных ФС легче сделать идемпотентными: bitmap или free-list? Объясните. ⭐⭐⭐
- Что означает атомарность изменений в ФС и почему транзакции решают эту проблему? ⭐⭐⭐
- В чём смысл границ begin transaction / commit для метаданных? ⭐⭐
- Что должно произойти при сбое **до commit** и **после commit**? ⭐⭐⭐
- Почему транзакции полезны не только для надёжности, но и для производительности? ⭐⭐⭐
- Почему журналирование стало стандартом де-факто в современных ОС? ⭐⭐
- Чем может отличаться «мощная система журналирования» NTFS от более простых вариантов? ⭐⭐⭐
- Почему ext3 получил распространение благодаря совместимости с ext2? ⭐⭐
- Почему journaling «по умолчанию» важен для пользовательского опыта на macOS? ⭐⭐

## Flash/SSD. Часть 1

- Какие фундаментальные физические отличия SSD от HDD влияют на дизайн ФС? ⭐⭐
- Почему чтение на SSD обычно быстрее записи, и какие последствия это имеет? ⭐⭐
- Почему страницу флеш-памяти нельзя перезаписать напрямую? ⭐⭐
- Что означает «стирание происходит блоками», и почему это усложняет обновления данных? ⭐⭐
- Что такое износ ячеек (P/E циклы) и почему он делает «лишние записи» вредными? ⭐⭐
- Чем отличаются **страница** и **flash block** с точки зрения гранулярности операций? ⭐⭐
- Почему запись возможна только в заранее стёртый блок? ⭐⭐
- Что фактически происходит при «перезаписи страницы» во флеш-памяти? ⭐⭐⭐
- Почему маленькие случайные записи особенно дорогие на SSD? ⭐⭐⭐
- Что такое FTL и почему он считается «мозгом SSD»? ⭐⭐
- Какую проблему решает отображение логических адресов на физические? ⭐⭐⭐
- Что такое wear-leveling и почему он невозможен без перемещения данных? ⭐⭐⭐
- Почему сборка мусора (garbage collection) нужна даже при наличии свободного места? ⭐⭐⭐
- Какие критерии могут влиять на выбор victim block при garbage collection? ⭐⭐⭐
- Почему реализация FTL является «секретной начинкой» SSD и что это означает для разработчиков ОС? ⭐⭐

## Flash/SSD. Часть 2

- В чём состоит семантический разрыв между файловой системой и SSD? ⭐⭐⭐
- Почему после удаления файла SSD «не знает», что логические блоки стали свободными? ⭐⭐
- Как команда TRIM помогает SSD работать эффективнее? ⭐⭐
- Почему SSD может работать и без TRIM, но хуже? В чём именно ухудшение? ⭐⭐⭐
- Почему SSD «любит последовательные записи» и как LFS это использует? ⭐⭐
- Почему отсутствие перезаписи in-place в LFS является преимуществом для флеш-памяти? ⭐⭐⭐
- Как LFS помогает распределять износ и взаимодействовать с FTL? ⭐⭐⭐
- Что такое проблема «бродячего дерева» (wandering tree) и почему она возникает? ⭐⭐⭐
- Почему обновление одной страницы может вызвать каскад обновлений метаданных? ⭐⭐⭐
- Какие метаданные обычно участвуют в каскаде (indirect blocks, inode, inode map) и почему? ⭐⭐⭐
- Как глобальные таблицы отображения уменьшают число каскадных обновлений? ⭐⭐⭐
- Почему F2FS считается «flash-friendly», и какие идеи делают её подходящей для Android/Linux? ⭐⭐⭐

## Virtual File System

- Почему на одном компьютере часто существует несколько файловых систем одновременно? ⭐
- В чём принципиальная разница подходов Windows (буквы дисков) и UNIX (единое дерево)? ⭐⭐
- Зачем нужен слой VFS и какие проблемы он решает для приложений? ⭐⭐
- Как VFS позволяет приложениям использовать один и тот же интерфейс (POSIX) для разных ФС? ⭐⭐⭐
- Что такое superblock в VFS и какую информацию он должен содержать? ⭐⭐
- Что такое v-node и почему он нужен как абстракция файла? ⭐⭐
- Почему VFS можно считать «объектно-ориентированным» по смыслу, даже если он написан на C? ⭐⭐
- Что хранит mount table и почему она критична для разбора путей? ⭐⭐
- Как связаны таблица файловых дескрипторов процесса и объекты VFS? ⭐⭐⭐
- Что означает «файловая система передаёт VFS таблицу указателей на функции»? ⭐⭐⭐
- Почему добавление новой ФС не требует переписывания VFS целиком? ⭐⭐
- Какие типы операций обязана реализовать ФС, чтобы быть совместимой с VFS? ⭐⭐⭐
- Опишите пошагово, как VFS обрабатывает `open("/usr/include/unistd.h")`. ⭐⭐⭐
- Как VFS определяет, что `/usr` является точкой монтирования другой файловой системы? ⭐⭐⭐
- Зачем VFS создаёт v-node и что в нём хранится? ⭐⭐⭐
- Как создаётся запись в таблице файловых дескрипторов и зачем она нужна? ⭐⭐
- Как по `fd` VFS находит нужный объект файла и вызывает метод конкретной ФС? ⭐⭐⭐
- Почему после получения fd приложение «не знает» на какой ФС лежит файл? ⭐⭐
- Какие преимущества это даёт для сетевых ФС, контейнеров и виртуализации? ⭐⭐⭐

## Windows vs UNIX/Linux

- Почему модель Windows «каждый диск — отдельная ФС» проще, но менее гибкая? ⭐⭐
- Как Windows выбирает драйвер файловой системы по пути `D:\data\file.txt`? ⭐⭐
- Как UNIX/Linux выбирает файловую систему для `open("/usr/bin/bash")`? ⭐⭐
- Почему взаимозаменяемость ФС проще реализуется в UNIX/Linux? ⭐⭐⭐
- Что такое UNC-путь и почему он существует параллельно буквенным дискам? ⭐⭐
- Почему UNC считается «отдельной вселенной» по отношению к `C:\`? ⭐⭐⭐
- В каких сценариях UNC особенно удобен для администрирования и CI/CD? ⭐⭐
- Почему некоторые программы плохо работают с UNC и как это проявляется? ⭐⭐⭐
- Что такое `\\?\` (extended-length path) и какие ограничения он снимает? ⭐⭐
- Почему при `\\?\` отключаются нормализация и интерпретация `.` и `..`? ⭐⭐⭐
- Какие риски и ошибки могут возникнуть при использовании путей `\\?\` в приложениях? ⭐⭐⭐
- Почему ограничение MAX_PATH=260 исторически появилось и почему оно до сих пор важно? ⭐⭐

## FUSE: файловая система в userspace

- Что такое FUSE и почему он делает разработку файловых систем безопаснее? ⭐⭐
- Как работает цепочка: приложение → системный вызов → модуль fuse → пользовательский демон → ответ? ⭐⭐⭐
- Почему возможность писать ФС без кода в ядре важна для переносимости и скорости разработки? ⭐⭐
- Почему FUSE можно запускать без root-прав, и что это даёт в безопасности? ⭐⭐⭐
- Какие примеры реальных задач решают sshfs, encfs/gocryptfs, ntfs-3g, archivemount, rclone mount? ⭐⭐
- Что такое callbacks в FUSE и почему архитектура строится вокруг них? ⭐⭐
- Какую роль играет `/dev/fuse` в взаимодействии ядра и userspace-ФС? ⭐⭐
- Почему FUSE медленнее, чем драйвер в ядре, и где именно возникают задержки? ⭐⭐⭐
- В каких случаях FUSE «незаменим», несмотря на меньшую производительность? ⭐⭐⭐

## Управление файловыми системами: размер блока ⭐

- Почему выбор размера блока — фундаментальный компромисс между скоростью и эффективностью хранения? ⭐⭐
- Объясните на примере файлов по 4KB, почему блок 8KB даёт потери, а блок 1KB увеличивает число операций. ⭐⭐⭐
- Почему большие блоки ускоряют чтение больших файлов, но ухудшают использование диска? ⭐⭐
- Какие факторы (тип нагрузки, тип диска, средний размер файлов) влияют на оптимальный размер блока? ⭐⭐⭐

## Управление свободным местом. Дисковые квоты

- Как работает связный список свободных блоков и почему он прост, но плохо масштабируется? ⭐⭐⭐
- Почему free-list может занимать много места на больших дисках? ⭐⭐
- Как bitmap кодирует свободное пространство и почему он считается очень компактным? ⭐⭐
- Почему bitmap удобнее для поиска последовательностей свободных блоков? ⭐⭐⭐
- Какие преимущества даёт хранение bitmap «по одной странице в памяти»? ⭐⭐⭐
- Зачем оптимизируют free-list через «pointer block наполовину заполненный» и как это уменьшает I/O? ⭐⭐⭐
- Зачем квоты нужны в многопользовательских системах и какие угрозы они предотвращают? ⭐⭐
- Чем ограничение «количество файлов» отличается от ограничения «количество блоков»? ⭐⭐
- В чём смысл soft limit и hard limit, и почему soft limit можно превышать временно? ⭐⭐
- Как связаны open files table и запись квоты при увеличении размера файла? ⭐⭐⭐
- Почему система предупреждает при входе в систему о превышении soft limit, а не обязательно сразу запрещает запись? ⭐⭐⭐

## Резервное копирование - 1

- Почему потеря файловой системы часто критичнее, чем потеря самого компьютера? ⭐⭐
- Какие две основные цели бэкапа отличают «катастрофу» от «ошибки пользователя»? ⭐⭐
- Почему бэкап — не тривиальная задача даже при наличии больших дешёвых носителей? ⭐⭐⭐
- Какие каталоги обычно исключают из бэкапа в UNIX и почему копирование `/dev` может быть опасным? ⭐⭐⭐
- Что такое инкрементальные копии и почему они усложняют восстановление? ⭐⭐⭐
- Почему компрессия в бэкапах может быть рискованной для целостности данных на носителе? ⭐⭐⭐
- Почему активная файловая система создаёт проблему консистентности при бэкапе? ⭐⭐⭐
- Зачем нужны snapshots и как они помогают делать консистентные бэкапы? ⭐⭐⭐
- Почему безопасность бэкап-носителей (офлайн хранение) часто важнее сетевой безопасности? ⭐⭐⭐
- Что такое физический dump и почему он очень быстрый и простой? ⭐⭐
- Почему физический dump плохо подходит для выборочного восстановления отдельных файлов? ⭐⭐
- Почему физический dump должен учитывать bad blocks и что произойдёт, если не учитывать? ⭐⭐⭐
- Что такое логический dump и почему он удобнее для восстановления структуры каталогов? ⭐⭐
- Почему при логическом dump копируются каталоги на пути к изменённым файлам? ⭐⭐⭐
- Опишите 4 фазы алгоритма логического дампа и объясните, почему порядок важен. ⭐⭐⭐
- Почему свободные блоки обычно не сохраняют в бэкапе и как они восстанавливаются? ⭐⭐⭐
- Почему hard links требуют специальной обработки при восстановлении из логического дампа? ⭐⭐⭐
- Что такое sparse file и почему нельзя восстанавливать «дырки» как реальные нулевые блоки? ⭐⭐⭐
- Почему спецфайлы (device nodes, pipes, sockets) нельзя копировать как обычные файлы? ⭐⭐⭐
- Почему для восстановления нужно применять full + incrementals строго в правильном порядке? ⭐⭐

## Резервное копирование - 2

- Чем File History отличается от Backup and Restore по целям и уровню копирования? ⭐⭐
- Почему VSS является ключом к консистентным бэкапам работающей Windows-системы? ⭐⭐⭐
- Как VSS разделяет роли Writer / Service / Provider и зачем это нужно? ⭐⭐⭐
- Почему некоторые файлы (pagefile.sys, hiberfil.sys) обычно исключают из бэкапа? ⭐⭐
- Какие проблемы создают NTFS junctions/symlinks/hardlinks при резервном копировании? ⭐⭐⭐
- Почему реестр Windows рассматривается как отдельный набор критичных файлов (hives) при восстановлении? ⭐⭐⭐
- Почему кэширование метаданных повышает производительность, но увеличивает риск несогласованности при сбое? ⭐⭐⭐
- Какие структуры ФС чаще всего повреждаются при внезапном выключении (inode, каталоги, bitmap/free-list) и почему? ⭐⭐⭐
- Что делает fsck при проверке блоков и зачем строятся таблицы «занятых» и «свободных» блоков? ⭐⭐⭐
- Какие три типичные ошибки блоков обнаруживает fsck и чем они опасны? ⭐⭐⭐
- Как fsck исправляет «потерянные блоки», «дубликаты в free list» и «блок принадлежит двум файлам»? ⭐⭐⭐
- Почему проверка link count особенно важна для предотвращения преждевременного удаления файла? ⭐⭐⭐
- Какие «подозрительные» признаки в правах доступа и структуре каталогов могут сигнализировать о проблемах безопасности? ⭐⭐⭐

## Производительность ФС

- Почему одиночный доступ к диску может быть медленнее доступа к памяти в миллион раз, и что это означает для дизайна ФС? ⭐⭐
- Как работает блоковый кэш и зачем нужна хеш-таблица для поиска блоков? ⭐⭐
- Почему точный LRU может быть нежелателен в файловых системах с точки зрения целостности? ⭐⭐⭐
- Какие блоки считаются критическими (inode, каталоги, indirect blocks) и почему их часто пишут на диск немедленно? ⭐⭐⭐
- Почему пользовательские данные можно держать в кэше дольше, но не бесконечно? ⭐⭐
- Сравните подходы: периодический sync в UNIX и write-through в ранних Windows. Какие плюсы и минусы? ⭐⭐⭐
- Чем отличается page cache от buffer cache и почему современные ОС их объединяют? ⭐⭐⭐
- Почему mmap и read могут использовать один и тот же кэш страниц и что это даёт? ⭐⭐⭐
- Почему опережающее чтение ускоряет последовательный доступ и как система «угадывает» шаблон? ⭐⭐⭐
- Почему после seek система считает доступ случайным и отключает read-ahead? ⭐⭐
- Почему для HDD важно размещать блоки рядом (в пределах цилиндра/группы), а для SSD это менее важно? ⭐⭐⭐
- Что такое cylinder groups (BSD FFS) и как они уменьшают количество seek? ⭐⭐⭐

## Дефрагментация, удаление файлов и шифрование

- Почему фрагментация появляется со временем даже на «идеально установленной» системе? ⭐⭐
- Как фрагментация влияет на производительность HDD и почему это связано с движением головки? ⭐⭐
- Опишите общий принцип работы дефрагментатора и почему ему нужна крупная свободная область. ⭐⭐⭐
- Почему некоторые файлы нельзя перемещать (pagefile.sys, hiberfil.sys, журнал NTFS) и какие последствия это имеет? ⭐⭐⭐
- Почему ext3/ext4 обычно меньше фрагментируются и редко требуют ручной дефрагментации? ⭐⭐⭐
- Почему дефрагментация на SSD не ускоряет работу и может ускорять износ? ⭐⭐⭐
- Почему удаление файла в ОС обычно не уничтожает данные физически? ⭐⭐
- Как злоумышленник может обойти права доступа ФС при физическом доступе к диску? ⭐⭐⭐
- Почему перезапись нулями на HDD иногда считается недостаточной, и что такое «остаточные следы»? ⭐⭐⭐
- Почему надёжное уничтожение данных на SSD сложнее из-за FTL и перенаправления записей? ⭐⭐⭐
- Какие источники «копий данных» кроме основного файла могут мешать безопасному удалению (кэш, snapshots, бэкапы)? ⭐⭐⭐
- Почему многократная перезапись (3–7 проходов) имеет смысл для HDD, но не гарантирует результат на SSD? ⭐⭐⭐
- Почему полное шифрование диска считается единственным практически надёжным способом защититься от восстановления данных? ⭐⭐⭐
- Какие условия должны выполняться, чтобы шифрование было реально безопасным (алгоритм, ключ, отсутствие утечек)? ⭐⭐⭐
- Как BitLocker использует VMK и какие способы разблокировки (пароль, recovery key, TPM) существуют? ⭐⭐⭐
- Почему самошифрующиеся диски (SED) могут быть опасны с точки зрения доверия, несмотря на высокую производительность? ⭐⭐⭐

## Сборка приложения: компиляция, ассемблирование, линковка

- Объясните полный путь превращения `main.cpp` и `foo.cpp` в `Program.exe`: какие артефакты появляются на каждом шаге и зачем они нужны? ⭐⭐
- Почему компилятор компилирует файлы `.cpp` по отдельности, а не сразу весь проект целиком? ⭐⭐
- Что такое объектный файл (`.obj/.o`) и какие виды информации в нём обычно присутствуют помимо машинного кода? ⭐⭐
- Что означает “неразрешённая ссылка” в объектном файле и почему это нормально до этапа линковки? ⭐⭐
- В чём различие между ошибками компиляции и ошибками линковки? Приведите типичные примеры. ⭐⭐
- Какие задачи выполняет линковщик при создании исполняемого файла, кроме “склеивания” объектных файлов? ⭐⭐
- Что такое “единое адресное пространство будущей программы” в контексте линковки? ⭐⭐⭐
- Почему библиотека времени выполнения (например, CRT) часто подключается автоматически, даже если разработчик явно её не указывал? ⭐⭐
- Как вы объясните роль `msvcrt.lib` в процессе сборки на Windows и почему её подключение показано на схеме? ⭐⭐
- Почему наличие заголовка (`Foo.h`) не означает, что функция уже “включена” в программу? ⭐⭐
- Дайте определение библиотеки и объясните, чем библиотека принципиально отличается от приложения. ⭐
- Какие проблемы разработки решают библиотеки, кроме “не писать один и тот же код заново”? ⭐⭐
- Почему повторное использование кода через библиотеки может одновременно повышать надёжность и увеличивать риски? ⭐⭐
- Какие типы “ресурсов” (кроме функций и классов) могут входить в библиотеку и как это влияет на архитектуру приложения? ⭐⭐
- Объясните разницу между статическими и динамическими библиотеками на уровне жизненного цикла: когда они “подключаются” и кем. ⭐⭐
- Почему статическая библиотека описывается как “архив объектных файлов”? ⭐
- Что именно происходит на этапе линковки, когда приложение использует статическую библиотеку? ⭐⭐
- Почему статическая компоновка делает программу независимой от наличия библиотеки на целевой машине? ⭐⭐
- Объясните, как статическая линковка влияет на размер файла программы и расход оперативной памяти в системе. ⭐⭐
- Почему обновление статической библиотеки требует пересборки всех программ, которые её используют? ⭐⭐
- Какие реальные сценарии оправдывают выбор статической линковки, несмотря на её недостатки? ⭐⭐
- Как линковщик “понимает”, что из статической библиотеки нужно взять `foo.obj`, но не брать `bar.obj`? ⭐⭐⭐
- Какие проблемы безопасности и сопровождения могут возникнуть, если статически линковать библиотеки в большое количество приложений? ⭐⭐⭐

## Динамические библиотеки, их плюсы и минусы.

- Объясните, что означает фраза: “программа содержит не сами функции из библиотеки, а только ссылки на них”. ⭐⭐
- Почему подключение динамической библиотеки переносит часть работы линковщика на время выполнения программы? ⭐⭐
- Чем отличаются форматы `.dll` и `.so` не только по расширению, но и по типичным механизмам загрузки/поиска? ⭐⭐
- Как динамическая линковка влияет на распространение приложения (deployment) и требования к окружению? ⭐⭐
- В каких случаях динамическая библиотека может быть подключена не при запуске, а позже? ⭐⭐
- Что такое “символ” в бинарном смысле и почему он важнее, чем исходный код функции? ⭐⭐
- Чем отличаются символы функций и символы данных с точки зрения загрузчика? ⭐⭐
- Что такое таблица импорта и какие данные она должна содержать, чтобы связывание стало возможным? ⭐⭐
- Что такое таблица экспорта и почему она является “витриной” библиотеки? ⭐
- Как загрузчик связывает импорт и экспорт: какие шаги он выполняет при старте процесса? ⭐⭐⭐
- Почему динамический загрузчик должен уметь разрешать зависимости библиотек друг от друга? ⭐⭐
- Что произойдёт, если импортируемый символ отсутствует в таблице экспорта найденной DLL/so? ⭐⭐
- Почему динамические библиотеки позволяют экономить память, и при каких условиях эта экономия максимальна? ⭐⭐
- Объясните, как несколько процессов могут использовать один и тот же код библиотеки одновременно, оставаясь изолированными. ⭐⭐⭐
- Почему обновление DLL/so может “починить” программу без пересборки — и почему это же может её сломать? ⭐⭐
- В чём заключается “сложность настройки окружения” для динамически связанного приложения? ⭐⭐
- Приведите пример ситуации, когда динамическая линковка создаёт проблему, которую статическая линковка устранила бы полностью. ⭐⭐
- Почему системные вызовы часто “упакованы” в библиотеки, а не вызываются приложением напрямую? ⭐⭐
- Как динамические библиотеки помогают модульности ОС и обновляемости компонентов? ⭐⭐
- Что такое runtime-библиотека языка и почему без неё приложение может не запуститься? ⭐⭐
- Почему плагины и расширения приложений почти всегда реализуются через динамическую загрузку? ⭐⭐
- Как динамические библиотеки связаны с драйверами устройств (даже если драйвер — это не всегда DLL/so)? ⭐⭐⭐

## Динамическая компоновка. Адресное пространство процесса

- Что такое импортная библиотека (`.lib`) в Windows и почему она не содержит реального кода функций? ⭐⭐
- Почему в `app.exe` появляется запись вида `Foo@Util.dll`, и что она означает на практике? ⭐⭐
- Объясните, что именно добавляет линковщик в исполняемый файл при динамической линковке вместо машинного кода функции. ⭐⭐⭐
- Почему приложение может успешно собраться, но не запуститься из-за DLL? ⭐⭐
- Объясните смысл конструкции `__imp__SomeFunction` и почему вызов идёт не напрямую. ⭐⭐⭐
- Что конкретно делает загрузчик, когда “заполняет таблицу импорта”? ⭐⭐⭐
- Почему после заполнения таблицы импорта вызовы становятся почти такими же быстрыми, как обычные? ⭐⭐
- Какие ошибки возможны на этом этапе и как они проявляются пользователю/разработчику? ⭐⭐
- Объясните, как секции `.text`, `.data`, `.rodata` отображаются из файлов в виртуальную память процесса. ⭐⭐
- Почему `.text` и `.rodata` обычно могут быть разделяемыми между процессами, а `.data` — нет? ⭐⭐⭐
- Какую роль играют куча и стек в общей картине памяти процесса, и почему они не являются “частью библиотеки”? ⭐⭐
- Почему библиотека в памяти выглядит “как обычная часть процесса”, хотя физически хранится в отдельном файле? ⭐⭐

## Релокация. PIC. Безопасность

- Почему динамическая библиотека не может всегда загружаться по одному фиксированному адресу? ⭐⭐
- Что такое базовый адрес библиотеки и почему он может отличаться при разных запусках и в разных процессах? ⭐⭐
- Опишите пошагово процесс релокации при загрузке библиотеки. ⭐⭐⭐
- Что хранится в таблице релокаций и почему без неё библиотека может не работать? ⭐⭐⭐
- Как ASLR усиливает необходимость релокации и PIC? ⭐⭐
- Почему релокация — это “цена за гибкость”, и где именно возникает накладная стоимость? ⭐⭐
- Объясните механизм, при котором две программы могут использовать один и тот же физический код `ole32.dll`. ⭐⭐⭐
- Почему модификация страниц `.text` разрушает совместное использование памяти между процессами? ⭐⭐⭐
- В каких случаях релокация приводит к тому, что библиотека перестаёт быть “разделяемой” по коду? ⭐⭐⭐
- Какие практические выгоды даёт совместное использование кода библиотек для запуска и работы системы? ⭐⭐
- Что такое PIC и почему он критически важен для современных динамических библиотек? ⭐⭐
- Как относительная адресация помогает коду оставаться корректным при перемещении в памяти? ⭐⭐
- Почему PIC уменьшает потребность в релокации именно в сегменте `.text`? ⭐⭐⭐
- Объясните, почему инструкция `CALL rel32` “дружит” с PIC лучше, чем абсолютные формы вызова. ⭐⭐⭐
- Зачем нужны дополнительные таблицы косвенного вызова (GOT/PLT/IAT) при вызове внешних функций? ⭐⭐⭐
- Объясните принцип работы ASLR и какие области памяти он рандомизирует. ⭐⭐
- Почему ASLR усложняет эксплуатацию уязвимостей, но не делает её невозможной? ⭐⭐⭐
- Почему старые программы могут “не поддерживать ASLR”, и что делает ОС в таких случаях? ⭐⭐⭐
- Объясните идею ROP: почему злоумышленнику не нужно внедрять собственный код? ⭐⭐⭐
- Что такое gadget и почему он почти всегда заканчивается инструкцией `ret`? ⭐⭐⭐
- Как динамические библиотеки увеличивают “поверхность атаки” для ROP? ⭐⭐⭐
- Почему NX/DEP плохо защищает от ROP, и какую роль тут играет повторное использование кода? ⭐⭐⭐
- Почему ASLR снижает эффективность ROP и что атакующему нужно, чтобы его обойти? ⭐⭐⭐

## Загрузка библиотек. Декорирование имён. ABI совместимость

- Чем отличается неявная загрузка библиотек при старте процесса от явной загрузки “по требованию”? ⭐⭐
- Опишите полный сценарий явной загрузки DLL в Windows: от загрузки до вызова функции и выгрузки. ⭐⭐
- Зачем в Windows существует `LoadLibraryEx`, если уже есть `LoadLibrary`? Какие задачи он решает? ⭐⭐⭐
- Что делает `GetProcAddress` и какие типичные ошибки допускают при использовании этой функции? ⭐⭐⭐
- Опишите полный сценарий явной загрузки `.so` в Linux через `dlopen/dlsym/dlclose`. ⭐⭐
- Что означает флаг `RTLD_LAZY`, какие плюсы он даёт и какие риски создаёт? ⭐⭐⭐
- Почему при работе с `dlopen` важно проверять ошибки и как обычно это делается? ⭐⭐
- Зачем существует `dlmopen` и какую проблему он пытается решить? ⭐⭐⭐
- Почему в C++ вообще существует name mangling, и какие возможности языка он поддерживает? ⭐⭐
- Почему один и тот же исходный код может экспортировать разные имена символов при разных компиляторах? ⭐⭐⭐
- Как name mangling влияет на возможность вызвать функцию через `dlsym/GetProcAddress`? ⭐⭐
- Почему “чистый экспорт C++-символов” плохо подходит для системных библиотек и внешних плагинов? ⭐⭐⭐
- Что такое ABI и почему совместимость ABI важнее совместимости исходников для DLL/so? ⭐⭐⭐
- Как `extern "C"` влияет на экспортируемые символы и почему это повышает переносимость? ⭐⭐
- Объясните, зачем нужен шаблон `#ifdef __cplusplus extern "C" {}` в заголовках библиотек. ⭐⭐
- Почему передача `std::string` через границу приложение ↔ DLL может быть опасной даже при одинаковом исходном коде? ⭐⭐⭐
- Как разные runtime-библиотеки приводят к разным пулам памяти и почему это критично для `new/delete`? ⭐⭐⭐
- Почему ошибка “выделили в DLL, освободили в приложении” может проявляться не сразу? ⭐⭐⭐
- В чём проблема несовместимости соглашений о вызовах функций при работе с библиотеками? ⭐⭐⭐
- Какие типы данных считаются наиболее безопасными для передачи через ABI-границу и почему? ⭐⭐
- Сформулируйте правило “кто выделяет — тот освобождает” и объясните, как оно снижает риск повреждения кучи. ⭐⭐
- Почему симметричные функции `AllocateMemory/FreeMemory` считаются хорошим стилем API? ⭐⭐
- Как можно безопасно “передать сложный объект”, не передавая его напрямую (обёртки/сериализация)? ⭐⭐⭐
- Что такое opaque handle (непрозрачный дескриптор) и почему он часто лучше, чем экспорт классов? ⭐⭐⭐
- Какие проблемы могут возникнуть при передаче структур, содержащих указатели, между приложением и DLL? ⭐⭐⭐

## Проблемы с DLL. Инструменты анализа

- Объясните феномен DLL Hell: почему он был особенно характерен для Windows 90-х и 2000-х? ⭐⭐
- Как перезапись DLL в системных каталогах могла ломать чужие приложения? ⭐⭐
- Почему отсутствие изоляции библиотек усиливает вероятность конфликтов версий? ⭐⭐
- Что такое Side-by-Side (SxS) загрузка и как манифест помогает выбрать нужную версию DLL? ⭐⭐⭐
- Почему “класть DLL рядом с exe” часто решает проблему, но создаёт новые недостатки? ⭐⭐
- Почему .NET исторически уменьшил проявления DLL Hell по сравнению с нативными приложениями? ⭐⭐
- В чём отличие dependency hell в Linux от DLL hell в Windows (по причинам и симптомам)? ⭐⭐⭐
- Как версионирование `.so` (например, `libfoo.so.1` и `libfoo.so.2`) помогает сосуществовать разным ABI? ⭐⭐⭐
- Какую роль играют менеджеры пакетов в предотвращении dependency hell и где их возможности заканчиваются? ⭐⭐⭐
- Почему `LD_LIBRARY_PATH` полезен для отладки, но может быть источником проблем и уязвимостей? ⭐⭐⭐
- Как контейнеризация решает проблемы зависимостей и почему это считается “тяжёлым”, но надёжным подходом? ⭐⭐
- Какие данные о загруженных DLL можно увидеть в Process Explorer и как это помогает расследовать проблемы? ⭐⭐
- Как с помощью Process Explorer можно заподозрить конфликт версий DLL или “подмену” библиотеки? ⭐⭐⭐
- Что показывает `ldd` и почему его вывод не всегда гарантирует, что программа реально запустится? ⭐⭐
- Чем полезны `readelf` и `objdump` для анализа ELF и зависимостей на уровне бинарника? ⭐⭐⭐
- Как `LD_DEBUG` помогает понять, где загрузчик ищет библиотеки и почему это важно при отладке? ⭐⭐⭐
- Что можно узнать из `/proc/<pid>/maps` о загруженных библиотеках и правах доступа страниц? ⭐⭐⭐
- Чем отличаются `strace` и `ltrace`, и какие вопросы про динамическую линковку они помогают ответить? ⭐⭐⭐
